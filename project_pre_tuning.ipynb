{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-08T13:47:50.508033Z",
     "start_time": "2025-12-08T13:47:50.496220Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "fd6c5c202d42c147",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-08T10:02:39.558152Z",
     "start_time": "2025-12-08T10:02:18.877346Z"
    }
   },
   "source": [
    "# loading the data\n",
    "df = pd.read_csv(\"loan.csv\")\n",
    "df.shape\n",
    "df_raw = df.copy()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tamar\\AppData\\Local\\Temp\\ipykernel_21616\\961585608.py:2: DtypeWarning: Columns (19,47,55,112,123,124,125,128,129,130,133,139,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"loan.csv\")\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "3812168bef44e6a9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-08T10:02:42.613123Z",
     "start_time": "2025-12-08T10:02:42.565234Z"
    }
   },
   "source": [
    "# -------------------------------------------------------------\n",
    "# Step 1: Create the 3-class target (common for both pipelines)\n",
    "# -------------------------------------------------------------\n",
    "def create_target(df):\n",
    "    \"\"\"\n",
    "    Creates the 3-class target:\n",
    "      - paid_on_time\n",
    "      - paid_late\n",
    "      - not_paid\n",
    "\n",
    "    Also computes helper columns needed to classify paid_late correctly.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%Y', errors='coerce')\n",
    "    df['last_pymnt_d'] = pd.to_datetime(df['last_pymnt_d'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "    # Extract loan term in months\n",
    "    df['term_months'] = df['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    # Approximate expected end date\n",
    "    df['expected_end'] = df['issue_d'] + pd.to_timedelta(df['term_months'] * 30, unit='D')\n",
    "\n",
    "    # Late fully paid flag\n",
    "    df['paid_late_flag'] = (\n",
    "        (df['loan_status'] == 'Fully Paid') &\n",
    "        (df['last_pymnt_d'] > df['expected_end'])\n",
    "    )\n",
    "\n",
    "    # Build target variable\n",
    "    df['target_3class'] = 'paid_on_time'\n",
    "    df.loc[df['paid_late_flag'], 'target_3class'] = 'paid_late'\n",
    "    df.loc[df['loan_status'].isin(['Charged Off', 'Default']), 'target_3class'] = 'not_paid'\n",
    "    # Remove '(future leakage + breaks categorical encoding)\n",
    "    df = df.drop(columns=['next_pymnt_d', 'paid_late_flag', 'last_pymnt_d'], errors='ignore')\n",
    "\n",
    "\n",
    "    return df[df['target_3class'].notna()].reset_index(drop=True)\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "651817f39d5a64fd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-08T10:02:43.068714Z",
     "start_time": "2025-12-08T10:02:43.002319Z"
    }
   },
   "source": [
    "# =============================================================\n",
    "# FULL CLEAN PIPELINE (30 variables clean)\n",
    "# =============================================================\n",
    "leakage_columns = [\n",
    "    'hardship_flag','debt_settlement_flag','total_pymnt','total_rec_prncp',\n",
    "    'total_rec_int','total_rec_late_fee','last_pymnt_d','last_pymnt_amnt',\n",
    "    'recoveries','collection_recovery_fee','out_prncp','total_pymnt_inv',\n",
    "    'out_prncp_inv','loan_status'\n",
    "]\n",
    "\n",
    "high_corr_drop_columns = [\n",
    "    'funded_amnt','funded_amnt_inv','installment',\n",
    "    'num_rev_tl_bal_gt_0','tot_hi_cred_lim',\n",
    "    'total_il_high_credit_limit','num_sats'\n",
    "]\n",
    "\n",
    "redundant_columns = ['policy_code','disbursement_method','chargeoff_within_12_mths', 'initial_list_status']\n",
    "\n",
    "\n",
    "def basic_clean1(df):\n",
    "    \"\"\"\n",
    "    First stage of full clean:\n",
    "    - Remove leakage, high correlation, and redundant columns\n",
    "    - Drop columns with >90% missing\n",
    "    - Clean term and emp_length\n",
    "    - Remove rows with missing target\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    cols_to_drop = [c for c in (leakage_columns + high_corr_drop_columns + redundant_columns) \n",
    "                    if c in clean_df.columns]\n",
    "    clean_df = clean_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Drop columns with excessive missingness\n",
    "    missing_pct = clean_df.isnull().mean() * 100\n",
    "    high_missing_cols = missing_pct[missing_pct > 90].index.tolist()\n",
    "    clean_df = clean_df.drop(columns=high_missing_cols)\n",
    "\n",
    "    # Remove \"Not Verified\"\n",
    "    clean_df = clean_df[clean_df['verification_status'] != 'Not Verified']\n",
    "    clean_df = clean_df.drop(columns=['verification_status'], errors='ignore')\n",
    "\n",
    "    # Clean term (36/60)\n",
    "    clean_df['term'] = clean_df['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    # Clean employee length\n",
    "    emp = clean_df['emp_length'].astype(str)\n",
    "    emp = emp.str.replace('< 1', '0', regex=False)\n",
    "    clean_df['emp_length'] = emp.str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    return clean_df[clean_df['target_3class'].notna()].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def basic_clean2(df):\n",
    "    \"\"\"\n",
    "    Second stage of full clean:\n",
    "    - Remove non-informative columns\n",
    "    - Normalize text columns\n",
    "    - Create engineered features (purpose_grouped, home_stability, credit_age_years, etc.)\n",
    "    - Remove raw columns after feature engineering\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    # Non-informative\n",
    "    for col in ['pymnt_plan','zip_code','collections_12_mths_ex_med']:\n",
    "        clean_df = clean_df.drop(columns=col, errors='ignore')\n",
    "\n",
    "    # Lowercase text columns\n",
    "    for col in ['purpose','home_ownership','addr_state','application_type','emp_title']:\n",
    "        if col in clean_df:\n",
    "            clean_df[col] = clean_df[col].astype(str).str.lower().str.strip()\n",
    "\n",
    "    # Grouped purpose variable\n",
    "    clean_df['purpose_grouped'] = clean_df['purpose'].replace({\n",
    "        'debt_consolidation':'debt','credit_card':'debt',\n",
    "        'home_improvement':'housing','house':'housing',\n",
    "        'small_business':'business',\n",
    "        'car':'personal','medical':'personal','vacation':'personal','moving':'personal',\n",
    "        'wedding':'personal','major_purchase':'personal',\n",
    "        'renewable_energy':'other','educational':'other','other':'other'\n",
    "    })\n",
    "\n",
    "    # Home stability\n",
    "    clean_df['home_stability'] = clean_df['home_ownership'].replace({\n",
    "        'mortgage':'stable','own':'stable',\n",
    "        'rent':'unstable','none':'unstable','other':'unstable'\n",
    "    })\n",
    "\n",
    "    # Credit age\n",
    "    years = clean_df['earliest_cr_line'].astype(str).str.extract(r'(\\d{4})')[0].astype(float)\n",
    "    clean_df['credit_age_years'] = datetime.now().year - years\n",
    "\n",
    "    # Create engineered count-like fields\n",
    "    clean_df['bad_records_count'] = clean_df[['pub_rec','pub_rec_bankruptcies','tax_liens']].sum(axis=1, min_count=1)\n",
    "    clean_df['recent_credit_activity'] = (\n",
    "        clean_df['inq_last_6mths'] + clean_df['num_tl_op_past_12m'] - (clean_df['mths_since_recent_inq']/12)\n",
    "    )\n",
    "\n",
    "    clean_df['total_balance_all'] = clean_df['tot_cur_bal'] + clean_df['total_bal_il']\n",
    "    clean_df['active_credit_accounts'] = clean_df['num_actv_bc_tl'] + clean_df['num_actv_rev_tl']\n",
    "\n",
    "    # Binary delinquency flag\n",
    "    clean_df['any_delinquency'] = (\n",
    "        (clean_df['num_accts_ever_120_pd']>0) |\n",
    "        (clean_df['num_tl_120dpd_2m']>0) |\n",
    "        (clean_df['num_tl_90g_dpd_24m']>0) |\n",
    "        (clean_df['num_tl_30dpd']>0) |\n",
    "        (clean_df['delinq_2yrs']>0)\n",
    "    ).astype(int)\n",
    "\n",
    "    clean_df['is_joint_app'] = clean_df['application_type'].str.contains('joint').astype(int)\n",
    "    clean_df['has_current_delinquency'] = (clean_df['acc_now_delinq']>0).astype(int)\n",
    "    clean_df['has_collections'] = (clean_df['tot_coll_amt']>0).astype(int)\n",
    "\n",
    "    # Drop raw columns after creating engineered features\n",
    "    columns_to_remove = [\n",
    "        'purpose','home_ownership','earliest_cr_line','application_type',\n",
    "        'acc_now_delinq','tot_coll_amt','title','emp_title',\n",
    "        'addr_state','issue_d','last_credit_pull_d','loan_status',\n",
    "        'days_late','open_acc','revol_bal','pub_rec','pub_rec_bankruptcies',\n",
    "        'tax_liens','inq_last_6mths','num_tl_op_past_12m','mths_since_recent_inq',\n",
    "        'tot_cur_bal','total_bal_il','num_actv_bc_tl','num_actv_rev_tl',\n",
    "        'num_accts_ever_120_pd','num_tl_120dpd_2m','num_tl_90g_dpd_24m',\n",
    "        'delinq_2yrs','num_tl_30dpd'\n",
    "    ]\n",
    "\n",
    "    clean_df = clean_df.drop(columns=[c for c in columns_to_remove if c in clean_df.columns])\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def basic_clean3(df):\n",
    "    \"\"\"\n",
    "    Final stage of full clean:\n",
    "    Removes noisy, sparse, redundant, or low-importance features.\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    low_importance = [\n",
    "        'open_act_il','open_il_12m','open_il_24m','open_rv_12m','open_rv_24m',\n",
    "        'open_acc_6m','inq_fi','total_cu_tl','acc_open_past_24mths'\n",
    "    ]\n",
    "\n",
    "    redundant = [\n",
    "        'num_bc_sats','num_bc_tl','num_il_tl','num_op_rev_tl',\n",
    "        'num_rev_accts','total_acc'\n",
    "    ]\n",
    "\n",
    "    sparse = [\n",
    "        'mths_since_last_record','mths_since_recent_bc_dlq',\n",
    "        'mths_since_recent_revol_delinq','percent_bc_gt_75'\n",
    "    ]\n",
    "\n",
    "    noisy = [\n",
    "        'mo_sin_old_il_acct','mo_sin_old_rev_tl_op',\n",
    "        'mo_sin_rcnt_rev_tl_op','mo_sin_rcnt_tl',\n",
    "        'mths_since_rcnt_il'\n",
    "    ]\n",
    "\n",
    "    optional = [\n",
    "        'mths_since_last_delinq','avg_cur_bal','max_bal_bc','all_util','il_util',\n",
    "        'inq_last_12m','pct_tl_nvr_dlq','mort_acc','total_bc_limit','total_acc'\n",
    "    ]\n",
    "\n",
    "    to_drop = low_importance + redundant + sparse + noisy + optional\n",
    "    clean_df = clean_df.drop(columns=[c for c in to_drop if c in clean_df.columns])\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def full_clean(df):\n",
    "    \"\"\"\n",
    "    Applies the full 3-stage cleaning process:\n",
    "    basic_clean1 → basic_clean2 → basic_clean3\n",
    "    \"\"\"\n",
    "    df1 = basic_clean1(df)\n",
    "    df2 = basic_clean2(df1)\n",
    "    df3 = basic_clean3(df2)\n",
    "    return df3"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "b82304b58741665d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-08T10:02:43.426189Z",
     "start_time": "2025-12-08T10:02:43.405575Z"
    }
   },
   "source": [
    "# =============================================================\n",
    "# MINIMAL CLEAN PIPELINE\n",
    "# Extracted from: data minimal clean.py\n",
    "# =============================================================\n",
    "def minimal_clean1(df):\n",
    "    \"\"\"\n",
    "    Minimal clean:\n",
    "    - Remove leakage\n",
    "    - Remove columns with >90% missing\n",
    "    - Remove one feature from each high-correlation (>0.95) pair\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    leakage = [\n",
    "        'hardship_flag','debt_settlement_flag','total_pymnt','total_rec_prncp',\n",
    "        'total_rec_int','total_rec_late_fee','last_pymnt_d','last_pymnt_amnt',\n",
    "        'recoveries','collection_recovery_fee','out_prncp','total_pymnt_inv',\n",
    "        'out_prncp_inv','loan_status','paid_late_flag'\n",
    "    ]\n",
    "    clean_df = clean_df.drop(columns=[c for c in leakage if c in clean_df.columns])\n",
    "\n",
    "    # Drop >90% missing\n",
    "    missing_pct = clean_df.isnull().mean() * 100\n",
    "    high_missing = missing_pct[missing_pct > 90].index.tolist()\n",
    "    clean_df = clean_df.drop(columns=high_missing)\n",
    "\n",
    "    # High-correlation removal\n",
    "    num = clean_df.select_dtypes(include='number').columns\n",
    "    if len(num) > 1:\n",
    "        corr = clean_df[num].corr().abs()\n",
    "        to_drop = set()\n",
    "\n",
    "        missing = clean_df[num].isnull().mean()\n",
    "        var = clean_df[num].var()\n",
    "\n",
    "        for i, c1 in enumerate(num):\n",
    "            for j, c2 in enumerate(num):\n",
    "                if j <= i:\n",
    "                    continue\n",
    "                if corr.loc[c1, c2] > 0.95:\n",
    "                    if missing[c1] > missing[c2]:\n",
    "                        to_drop.add(c1)\n",
    "                    elif missing[c2] > missing[c1]:\n",
    "                        to_drop.add(c2)\n",
    "                    else:\n",
    "                        to_drop.add(c1 if var[c1] < var[c2] else c2)\n",
    "\n",
    "        clean_df = clean_df.drop(columns=list(to_drop))\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def minimal_clean2(df):\n",
    "    \"\"\"\n",
    "    Additional minimal cleaning:\n",
    "    Convert term and emp_length to numeric, extract issue year.\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    clean_df['term'] = clean_df['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    emp = clean_df['emp_length'].astype(str)\n",
    "    emp = emp.str.replace('< 1', '0', regex=False)\n",
    "    clean_df['emp_length'] = emp.str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    clean_df['issue_year'] = clean_df['issue_d'].astype(str).str.extract(r'(\\d{4})')[0].astype(float)\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def minimal_clean(df):\n",
    "    \"\"\"\n",
    "    Applies minimal_clean1 → minimal_clean2\n",
    "    \"\"\"\n",
    "    df1 = minimal_clean1(df)\n",
    "    df2 = minimal_clean2(df1)\n",
    "    return df2[df2[\"target_3class\"].notna()].reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "0c4fda86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T10:02:44.110473Z",
     "start_time": "2025-12-08T10:02:44.097375Z"
    }
   },
   "source": [
    "def make_preprocess(df):\n",
    "    \"\"\"\n",
    "    Builds preprocessing:\n",
    "      - Time since event columns (mths_since_*):\n",
    "          * Impute missing with 0  (interpreted as 'no event')\n",
    "          * Add missing indicator\n",
    "          * Scale\n",
    "      - Other numeric columns:\n",
    "          * Impute missing with median\n",
    "          * Add missing indicator\n",
    "          * Scale\n",
    "      - Categorical columns (including ordinal-like text):\n",
    "          * Impute missing with string 'missing'\n",
    "          * One-hot encode (missing becomes its own category)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Identify column groups ---\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    time_cols = [col for col in numeric_cols if col.startswith(\"mths_since_\")]\n",
    "    num_regular = list(set(numeric_cols) - set(time_cols))\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "    # --- 2. Define pipelines for each group ---\n",
    "    time_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0, add_indicator=True)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    numeric_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    # --- 3. Combine all in ColumnTransformer ---\n",
    "\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"time\", time_pipeline, time_cols),\n",
    "            (\"num\", numeric_pipeline, num_regular),\n",
    "            (\"cat\", categorical_pipeline, categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\" \n",
    "    )\n",
    "\n",
    "    return transformer\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "7319fc89de0ec9e8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-08T10:02:44.690240Z",
     "start_time": "2025-12-08T10:02:44.657878Z"
    }
   },
   "source": [
    "def build_model_pipeline(preprocess):\n",
    "    \"\"\"\n",
    "    Combines preprocessing + RandomForest classifier into a single Pipeline.\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "\n",
    "    return pipe"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "39f563a01f88e506",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-08T10:02:57.806215Z",
     "start_time": "2025-12-08T10:02:57.759592Z"
    }
   },
   "source": [
    "def run_experiment(df, title, model_type):\n",
    "    \"\"\"\n",
    "    Runs training + test split + preprocessing + model training.\n",
    "    model_type: \"random_forest\", \"logistic\", \"xgboost\"\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Running: {title} ({model_type}) ===\")\n",
    "\n",
    "    # Split into features and target\n",
    "    X = df.drop(columns=[\"target_3class\"])\n",
    "    y = df[\"target_3class\"]\n",
    "    \n",
    "        # Convert target to numeric for XGBoost\n",
    "    if model_type == \"xgboost\":\n",
    "        y = y.astype(\"category\").cat.codes\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        stratify=y,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Build preprocessing based only on the feature matrix\n",
    "    preprocess = make_preprocess(X_train)\n",
    "\n",
    "    # Choose model\n",
    "    if model_type == \"random_forest\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=650,\n",
    "            max_depth=None,\n",
    "            min_samples_split=7,\n",
    "            min_samples_leaf=1,\n",
    "            max_features=0.5,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_type == \"xgboost\":\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=550,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=8,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            min_child_weight=1,\n",
    "            gamma=0.6,\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\"   # FAST + GPU compatible\n",
    "        )\n",
    "\n",
    "    elif model_type == \"logistic\":\n",
    "        model = LogisticRegression(\n",
    "            penalty='l2',\n",
    "            max_iter=2000,\n",
    "            l1_ratio=0.8,\n",
    "            class_weight=\"balanced\",\n",
    "            C=0.05\n",
    "        )\n",
    "\n",
    "    # Create pipeline\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", preprocess),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Fit\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # ---------- Evaluation ----------\n",
    "    # תחזיות על קבוצת ה-test\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # 1. Accuracy – אחוז הדוגמאות שנחזו נכון\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # 2. Macro F1 – ממוצע F1 לכל המחלקות\n",
    "    macro_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Macro F1:\", macro_f1)\n",
    "\n",
    "    # 3. Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # 4. Classification Report – Precision / Recall / F1 לכל מחלקה\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return acc, pipe"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "cfd9efa3e2ed2849",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-12-08T10:00:29.041987100Z",
     "start_time": "2025-12-06T21:16:30.424409Z"
    }
   },
   "source": [
    "# =============================================================\n",
    "# Run 3 models on both Full and Minimal datasets\n",
    "# =============================================================\n",
    "df_with_target = create_target(df_raw)\n",
    "\n",
    "df_sample = df_with_target.sample(100000, random_state=42)\n",
    "\n",
    "df_full_sample = full_clean(df_sample.copy())\n",
    "df_minimal_sample = minimal_clean(df_sample.copy())\n",
    "\n",
    "\n",
    "models = [\"random_forest\", \"xgboost\", \"logistic\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(\"\\n=============================================================\")\n",
    "    print(f\"MODEL: {model_name.upper()}\")\n",
    "    print(\"=============================================================\")\n",
    "\n",
    "    # ---- FULL CLEAN ----\n",
    "    acc_full, model_full = run_experiment(\n",
    "        df_full_sample,\n",
    "        f\"Full Clean Sample - {model_name}\",\n",
    "        model_type=model_name\n",
    "    )\n",
    "\n",
    "    # ---- MINIMAL CLEAN ----\n",
    "    acc_minimal, model_minimal = run_experiment(\n",
    "        df_minimal_sample,\n",
    "        f\"Minimal Clean Sample - {model_name}\",\n",
    "        model_type=model_name\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    results[(model_name, \"full\")] = acc_full\n",
    "    results[(model_name, \"minimal\")] = acc_minimal\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# Print Summary Table\n",
    "# =============================================================\n",
    "print(\"\\n\\n==================== SUMMARY ====================\\n\")\n",
    "print(\"{:<20} {:<15} {:<15}\".format(\"Model\", \"Full Clean\", \"Minimal Clean\"))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name in models:\n",
    "    print(\"{:<20} {:<15.4f} {:<15.4f}\".format(\n",
    "        model_name,\n",
    "        results[(model_name, \"full\")],\n",
    "        results[(model_name, \"minimal\")]\n",
    "    ))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### parameter tuning",
   "id": "8d7e499b3c3805c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T10:13:39.956842Z",
     "start_time": "2025-12-08T10:13:24.210736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#checking the number of values from each column:\n",
    "df_with_target = create_target(df_raw)\n",
    "df_with_target[\"target_3class\"].value_counts()"
   ],
   "id": "efeae3c45b4c87ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_3class\n",
       "paid_on_time    1788541\n",
       "not_paid         261686\n",
       "paid_late        210441\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--- random forest ---",
   "id": "c6a364017053cb9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T13:14:28.125798Z",
     "start_time": "2025-12-08T13:13:10.729476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load data\n",
    "df_tuning = df.copy()\n",
    "\n",
    "# 2. Create target\n",
    "df_with_target_tuning = create_target(df_tuning)\n",
    "\n",
    "# 3. Choose full or minimal clean\n",
    "df_clean_tuning = minimal_clean(df_with_target_tuning)     # or minimal_clean(df_with_target)\n",
    "\n",
    "# 4. Make balanced N-row dataset\n",
    "def make_balanced_sample(df, N, target_col=\"target_3class\", random_state=42):\n",
    "    k = df[target_col].nunique()\n",
    "    per_class = N // k\n",
    "\n",
    "    balanced = (\n",
    "        df.groupby(target_col)\n",
    "          .apply(lambda g: g.sample(per_class, random_state=random_state))\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    remaining = N - len(balanced)\n",
    "    if remaining > 0:\n",
    "        extra = df.sample(remaining, random_state=random_state)\n",
    "        balanced = pd.concat([balanced, extra], ignore_index=True)\n",
    "\n",
    "    return balanced\n",
    "\n",
    "df_balanced_tuning = make_balanced_sample(df_clean_tuning, 100000)\n",
    "\n",
    "# 5. Split into X and y\n",
    "X = df_balanced_tuning.drop(columns=[\"target_3class\"])\n",
    "y = df_balanced_tuning[\"target_3class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6. Build preprocessing on X_train only\n",
    "preprocess_tuning = make_preprocess(X_train)\n",
    "\n"
   ],
   "id": "f6324b1eb16cb65f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tamar\\AppData\\Local\\Temp\\ipykernel_21616\\2875091686.py:17: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(per_class, random_state=random_state))\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T13:15:41.495859Z",
     "start_time": "2025-12-08T13:14:32.788956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rf_pipe_tuning = Pipeline([\n",
    "    (\"pre\", preprocess_tuning),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_pipe_tuning.fit(X_train, y_train)\n",
    "y_pred = rf_pipe_tuning.predict(X_test)\n",
    "\n",
    "baseline_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"Baseline Macro F1:\", baseline_f1)"
   ],
   "id": "1be4a7761ac4c3c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Macro F1: 0.6757124296798948\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T13:14:30.480255Z",
     "start_time": "2025-12-08T13:14:30.468321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rf_params_coarse = {\n",
    "    # We tune in phases so the computer can handle it. After deciding on each phase, moving on to the next one.\n",
    "\n",
    "    # first:\n",
    "    \"clf__max_depth\": [None, 10, 20, 30, 50],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 4, 8,16],\n",
    "    \"clf__n_estimators\": [200, 400, 600, 800, 1200],\n",
    "\n",
    "\n",
    "    # second\n",
    "    \"clf__max_features\": [\"sqrt\", \"log2\", 0.2, 0.3, 0.5, 0.7, 1.0],\n",
    "    \"clf__n_estimators\": [300, 500, 800],\n",
    "    \"clf__max_features\": [\"sqrt\", \"log2\", 0.3, 0.5, 1.0],\n",
    "\n",
    "\n",
    "    #third\n",
    "    \"clf__min_samples_split\": [2, 5, 10, 20],\n",
    "    \"clf__class_weight\": [None, \"balanced\"]\n",
    "}"
   ],
   "id": "f7ecb260c0d0eccc",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rf_pipe = Pipeline([\n",
    "    (\"pre\", preprocess_tuning),\n",
    "    (\"clf\", RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "rf_search_coarse = RandomizedSearchCV(\n",
    "    estimator=rf_pipe,\n",
    "    param_distributions=rf_params_coarse,\n",
    "    n_iter=40,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_search_coarse.fit(X_train, y_train)"
   ],
   "id": "4f8b6722955f4073"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Best coarse params:\")\n",
    "print(rf_search_coarse.best_params_)\n",
    "\n",
    "print(\"Best CV f1_macro:\")\n",
    "print(rf_search_coarse.best_score_)"
   ],
   "id": "b7d8a7e370025838"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--- xg boost ---",
   "id": "55afdb28491a60f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:14:36.730273Z",
     "start_time": "2025-12-08T16:14:36.698656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_train = y_train.astype(\"category\").cat.codes\n",
    "y_test = y_test.astype(\"category\").cat.codes"
   ],
   "id": "7908434165118e4c",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:14:49.447087Z",
     "start_time": "2025-12-08T16:14:40.839830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xgb_pipe_tuning = Pipeline([\n",
    "    (\"pre\", preprocess_tuning),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=y.nunique(),\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_pipe_tuning.fit(X_train, y_train)\n",
    "y_pred = xgb_pipe_tuning.predict(X_test)\n",
    "baseline_xgb_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"Baseline XGB Macro F1:\", baseline_xgb_f1)"
   ],
   "id": "55a5dce62d9f53f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline XGB Macro F1: 0.7449341563021282\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:20:02.850193Z",
     "start_time": "2025-12-08T16:20:02.842870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xgb_params_coarse = {\n",
    "# We tune in phases so the computer can handle it. After deciding on each phase, moving on to the next one.\n",
    "\n",
    "    #first\n",
    "    \"clf__n_estimators\":        [200, 400, 800, 1200],\n",
    "    \"clf__learning_rate\":       [0.01, 0.05, 0.1],\n",
    "    \"clf__max_depth\":           [3, 7, 10, None],\n",
    "\n",
    "    #second\n",
    "    \"clf__min_child_weight\":    [1, 3, 5],\n",
    "    \"clf__subsample\":           [0.6, 0.8, 1.0],\n",
    "    \"clf__colsample_bytree\":    [0.6, 0.8, 1.0],\n",
    "}"
   ],
   "id": "165acb9683dddd04",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "xgb_search_coarse = RandomizedSearchCV(\n",
    "    estimator=xgb_pipe_tuning,\n",
    "    param_distributions=xgb_params_coarse,\n",
    "    n_iter=40,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_search_coarse.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best coarse params (XGB):\")\n",
    "print(xgb_search_coarse.best_params_)\n",
    "print(\"Best CV f1_macro (XGB):\")\n",
    "print(xgb_search_coarse.best_score_)"
   ],
   "id": "8937b31d48905be5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--- logistic regression ---",
   "id": "b5abbecab4f9bf5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:23:02.984677Z",
     "start_time": "2025-12-08T16:21:28.789210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_pipe_tuning = Pipeline([\n",
    "    (\"pre\", preprocess_tuning),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "lr_pipe_tuning.fit(X_train, y_train)\n",
    "y_pred = lr_pipe_tuning.predict(X_test)\n",
    "baseline_lr_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"Baseline LR Macro F1:\", baseline_lr_f1)"
   ],
   "id": "8404e46fe46b72f1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tamar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline LR Macro F1: 0.7178864947612209\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:24:11.359664Z",
     "start_time": "2025-12-08T16:24:11.331150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr_params_coarse = {\n",
    "# We tune in phases so the computer can handle it. After deciding on each phase, moving on to the next one.\n",
    "\n",
    "    #first\n",
    "    \"clf__C\":             [1e-3, 1e-2, 1e-1, 1, 5, 10],\n",
    "    \"clf__solver\":       [\"lbfgs\", \"saga\"],\n",
    "    \"clf__penalty\":      [\"l2\", \"elasticnet\"],\n",
    "\n",
    "    #second\n",
    "    \"clf__l1_ratio\":     [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "    \"clf__max_iter\":     [1000, 2000, 4000],\n",
    "}"
   ],
   "id": "603cc8a4703b9745",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lr_search_coarse = RandomizedSearchCV(\n",
    "    estimator=lr_pipe_tuning,\n",
    "    param_distributions=lr_params_coarse,\n",
    "    n_iter=40,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lr_search_coarse.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best coarse params (LR):\")\n",
    "print(lr_search_coarse.best_params_)\n",
    "print(\"Best CV f1_macro (LR):\")\n",
    "print(lr_search_coarse.best_score_)"
   ],
   "id": "8fd501ebd113b12f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3acb1a392c5529fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f1ad86636780c61d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4df21b57088a63bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_full_sample[\"target_3class\"].value_counts()\n",
    "df_full_sample[\"target_3class\"].value_counts(normalize=True)"
   ],
   "id": "117827833b07491f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_minimal_sample[\"target_3class\"].value_counts()\n",
    "df_minimal_sample[\"target_3class\"].value_counts(normalize=True)"
   ],
   "id": "7a86cb57b0bd053e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
