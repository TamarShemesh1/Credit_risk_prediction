{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-24T20:12:40.054442Z",
     "start_time": "2025-12-24T20:12:39.774342Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "fd6c5c202d42c147",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-24T20:13:10.467674Z",
     "start_time": "2025-12-24T20:12:40.076842Z"
    }
   },
   "source": [
    "# loading the data\n",
    "df = pd.read_csv(\"loan.csv\")\n",
    "df.shape\n",
    "df_raw = df.copy()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tamar\\AppData\\Local\\Temp\\ipykernel_19020\\961585608.py:2: DtypeWarning: Columns (19,47,55,112,123,124,125,128,129,130,133,139,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"loan.csv\")\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3812168bef44e6a9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-24T20:13:33.267088Z",
     "start_time": "2025-12-24T20:13:33.257585Z"
    }
   },
   "source": [
    "# -------------------------------------------------------------\n",
    "# Step 1: Create the 3-class target (common for both pipelines)\n",
    "# -------------------------------------------------------------\n",
    "def create_target(df):\n",
    "    \"\"\"\n",
    "    Creates the 3-class target:\n",
    "      - paid_on_time\n",
    "      - paid_late\n",
    "      - not_paid\n",
    "\n",
    "    Also computes helper columns needed to classify paid_late correctly.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%Y', errors='coerce')\n",
    "    df['last_pymnt_d'] = pd.to_datetime(df['last_pymnt_d'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "    # Extract loan term in months\n",
    "    df['term_months'] = df['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    # Approximate expected end date\n",
    "    df['expected_end'] = df['issue_d'] + pd.to_timedelta(df['term_months'] * 30, unit='D')\n",
    "\n",
    "    # Late fully paid flag\n",
    "    df['paid_late_flag'] = (\n",
    "        (df['loan_status'] == 'Fully Paid') &\n",
    "        (df['last_pymnt_d'] > df['expected_end'])\n",
    "    )\n",
    "\n",
    "    # Build target variable\n",
    "    df['target_3class'] = 'paid_on_time'\n",
    "    df.loc[df['paid_late_flag'], 'target_3class'] = 'paid_late'\n",
    "    df.loc[df['loan_status'].isin(['Charged Off', 'Default']), 'target_3class'] = 'not_paid'\n",
    "    # Remove '(future leakage + breaks categorical encoding)\n",
    "    df = df.drop(columns=['next_pymnt_d', 'paid_late_flag', 'last_pymnt_d'], errors='ignore')\n",
    "\n",
    "\n",
    "    return df[df['target_3class'].notna()].reset_index(drop=True)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "651817f39d5a64fd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-24T20:13:33.866572Z",
     "start_time": "2025-12-24T20:13:33.844530Z"
    }
   },
   "source": [
    "# =============================================================\n",
    "# FULL CLEAN PIPELINE (30 variables clean)\n",
    "# =============================================================\n",
    "leakage_columns = [\n",
    "    'hardship_flag','debt_settlement_flag','total_pymnt','total_rec_prncp',\n",
    "    'total_rec_int','total_rec_late_fee','last_pymnt_d','last_pymnt_amnt',\n",
    "    'recoveries','collection_recovery_fee','out_prncp','total_pymnt_inv',\n",
    "    'out_prncp_inv','loan_status'\n",
    "]\n",
    "\n",
    "high_corr_drop_columns = [\n",
    "    'funded_amnt','funded_amnt_inv','installment',\n",
    "    'num_rev_tl_bal_gt_0','tot_hi_cred_lim',\n",
    "    'total_il_high_credit_limit','num_sats'\n",
    "]\n",
    "\n",
    "redundant_columns = ['policy_code','disbursement_method','chargeoff_within_12_mths', 'initial_list_status']\n",
    "\n",
    "\n",
    "def basic_clean1(df):\n",
    "    \"\"\"\n",
    "    First stage of full clean:\n",
    "    - Remove leakage, high correlation, and redundant columns\n",
    "    - Drop columns with >90% missing\n",
    "    - Clean term and emp_length\n",
    "    - Remove rows with missing target\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    cols_to_drop = [c for c in (leakage_columns + high_corr_drop_columns + redundant_columns) \n",
    "                    if c in clean_df.columns]\n",
    "    clean_df = clean_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Drop columns with excessive missingness\n",
    "    missing_pct = clean_df.isnull().mean() * 100\n",
    "    high_missing_cols = missing_pct[missing_pct > 90].index.tolist()\n",
    "    clean_df = clean_df.drop(columns=high_missing_cols)\n",
    "\n",
    "    # Remove \"Not Verified\"\n",
    "    clean_df = clean_df[clean_df['verification_status'] != 'Not Verified']\n",
    "    clean_df = clean_df.drop(columns=['verification_status'], errors='ignore')\n",
    "\n",
    "    # Clean term (36/60)\n",
    "    clean_df['term'] = clean_df['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    # Clean employee length\n",
    "    emp = clean_df['emp_length'].astype(str)\n",
    "    emp = emp.str.replace('< 1', '0', regex=False)\n",
    "    clean_df['emp_length'] = emp.str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    return clean_df[clean_df['target_3class'].notna()].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def basic_clean2(df):\n",
    "    \"\"\"\n",
    "    Second stage of full clean:\n",
    "    - Remove non-informative columns\n",
    "    - Normalize text columns\n",
    "    - Create engineered features (purpose_grouped, home_stability, credit_age_years, etc.)\n",
    "    - Remove raw columns after feature engineering\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    # Non-informative\n",
    "    for col in ['pymnt_plan','zip_code','collections_12_mths_ex_med']:\n",
    "        clean_df = clean_df.drop(columns=col, errors='ignore')\n",
    "\n",
    "    # Lowercase text columns\n",
    "    for col in ['purpose','home_ownership','addr_state','application_type','emp_title']:\n",
    "        if col in clean_df:\n",
    "            clean_df[col] = clean_df[col].astype(str).str.lower().str.strip()\n",
    "\n",
    "    # Grouped purpose variable\n",
    "    clean_df['purpose_grouped'] = clean_df['purpose'].replace({\n",
    "        'debt_consolidation':'debt','credit_card':'debt',\n",
    "        'home_improvement':'housing','house':'housing',\n",
    "        'small_business':'business',\n",
    "        'car':'personal','medical':'personal','vacation':'personal','moving':'personal',\n",
    "        'wedding':'personal','major_purchase':'personal',\n",
    "        'renewable_energy':'other','educational':'other','other':'other'\n",
    "    })\n",
    "\n",
    "    # Home stability\n",
    "    clean_df['home_stability'] = clean_df['home_ownership'].replace({\n",
    "        'mortgage':'stable','own':'stable',\n",
    "        'rent':'unstable','none':'unstable','other':'unstable'\n",
    "    })\n",
    "\n",
    "    # Credit age\n",
    "    years = clean_df['earliest_cr_line'].astype(str).str.extract(r'(\\d{4})')[0].astype(float)\n",
    "    clean_df['credit_age_years'] = datetime.now().year - years\n",
    "\n",
    "    # Create engineered count-like fields\n",
    "    clean_df['bad_records_count'] = clean_df[['pub_rec','pub_rec_bankruptcies','tax_liens']].sum(axis=1, min_count=1)\n",
    "    clean_df['recent_credit_activity'] = (\n",
    "        clean_df['inq_last_6mths'] + clean_df['num_tl_op_past_12m'] - (clean_df['mths_since_recent_inq']/12)\n",
    "    )\n",
    "\n",
    "    clean_df['total_balance_all'] = clean_df['tot_cur_bal'] + clean_df['total_bal_il']\n",
    "    clean_df['active_credit_accounts'] = clean_df['num_actv_bc_tl'] + clean_df['num_actv_rev_tl']\n",
    "\n",
    "    # Binary delinquency flag\n",
    "    clean_df['any_delinquency'] = (\n",
    "        (clean_df['num_accts_ever_120_pd']>0) |\n",
    "        (clean_df['num_tl_120dpd_2m']>0) |\n",
    "        (clean_df['num_tl_90g_dpd_24m']>0) |\n",
    "        (clean_df['num_tl_30dpd']>0) |\n",
    "        (clean_df['delinq_2yrs']>0)\n",
    "    ).astype(int)\n",
    "\n",
    "    clean_df['is_joint_app'] = clean_df['application_type'].str.contains('joint').astype(int)\n",
    "    clean_df['has_current_delinquency'] = (clean_df['acc_now_delinq']>0).astype(int)\n",
    "    clean_df['has_collections'] = (clean_df['tot_coll_amt']>0).astype(int)\n",
    "\n",
    "    # Drop raw columns after creating engineered features\n",
    "    columns_to_remove = [\n",
    "        'purpose','home_ownership','earliest_cr_line','application_type',\n",
    "        'acc_now_delinq','tot_coll_amt','title','emp_title',\n",
    "        'addr_state','issue_d','last_credit_pull_d','loan_status',\n",
    "        'days_late','open_acc','revol_bal','pub_rec','pub_rec_bankruptcies',\n",
    "        'tax_liens','inq_last_6mths','num_tl_op_past_12m','mths_since_recent_inq',\n",
    "        'tot_cur_bal','total_bal_il','num_actv_bc_tl','num_actv_rev_tl',\n",
    "        'num_accts_ever_120_pd','num_tl_120dpd_2m','num_tl_90g_dpd_24m',\n",
    "        'delinq_2yrs','num_tl_30dpd'\n",
    "    ]\n",
    "\n",
    "    clean_df = clean_df.drop(columns=[c for c in columns_to_remove if c in clean_df.columns])\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def basic_clean3(df):\n",
    "    \"\"\"\n",
    "    Final stage of full clean:\n",
    "    Removes noisy, sparse, redundant, or low-importance features.\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    low_importance = [\n",
    "        'open_act_il','open_il_12m','open_il_24m','open_rv_12m','open_rv_24m',\n",
    "        'open_acc_6m','inq_fi','total_cu_tl','acc_open_past_24mths'\n",
    "    ]\n",
    "\n",
    "    redundant = [\n",
    "        'num_bc_sats','num_bc_tl','num_il_tl','num_op_rev_tl',\n",
    "        'num_rev_accts','total_acc'\n",
    "    ]\n",
    "\n",
    "    sparse = [\n",
    "        'mths_since_last_record','mths_since_recent_bc_dlq',\n",
    "        'mths_since_recent_revol_delinq','percent_bc_gt_75'\n",
    "    ]\n",
    "\n",
    "    noisy = [\n",
    "        'mo_sin_old_il_acct','mo_sin_old_rev_tl_op',\n",
    "        'mo_sin_rcnt_rev_tl_op','mo_sin_rcnt_tl',\n",
    "        'mths_since_rcnt_il'\n",
    "    ]\n",
    "\n",
    "    optional = [\n",
    "        'mths_since_last_delinq','avg_cur_bal','max_bal_bc','all_util','il_util',\n",
    "        'inq_last_12m','pct_tl_nvr_dlq','mort_acc','total_bc_limit','total_acc'\n",
    "    ]\n",
    "\n",
    "    to_drop = low_importance + redundant + sparse + noisy + optional\n",
    "    clean_df = clean_df.drop(columns=[c for c in to_drop if c in clean_df.columns])\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def full_clean(df):\n",
    "    \"\"\"\n",
    "    Applies the full 3-stage cleaning process:\n",
    "    basic_clean1 → basic_clean2 → basic_clean3\n",
    "    \"\"\"\n",
    "    df1 = basic_clean1(df)\n",
    "    df2 = basic_clean2(df1)\n",
    "    df3 = basic_clean3(df2)\n",
    "    return df3"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "b82304b58741665d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-24T20:13:34.252182Z",
     "start_time": "2025-12-24T20:13:34.241388Z"
    }
   },
   "source": [
    "# =============================================================\n",
    "# MINIMAL CLEAN PIPELINE\n",
    "# =============================================================\n",
    "def minimal_clean1(df):\n",
    "    \"\"\"\n",
    "    Minimal clean:\n",
    "    - Remove leakage\n",
    "    - Remove columns with >90% missing\n",
    "    - Remove one feature from each high-correlation (>0.95) pair\n",
    "    - Extreme-value capping\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    clean_df.drop(columns = [\"pymnt_plan\", \"policy_code\"])  #fixed values columns\n",
    "\n",
    "    # deleting non-verified data\n",
    "    clean_df = clean_df.drop(clean_df[clean_df[\"verification_status\"] == \"Not Verified\"].index)\n",
    "\n",
    "    leakage = [\n",
    "        'hardship_flag','debt_settlement_flag','total_pymnt','total_rec_prncp',\n",
    "        'total_rec_int','total_rec_late_fee','last_pymnt_d','last_pymnt_amnt',\n",
    "        'recoveries','collection_recovery_fee','out_prncp','total_pymnt_inv',\n",
    "        'out_prncp_inv','loan_status','paid_late_flag'\n",
    "    ]\n",
    "    clean_df = clean_df.drop(columns=[c for c in leakage if c in clean_df.columns])\n",
    "\n",
    "    # Drop >90% missing\n",
    "    missing_pct = clean_df.isnull().mean() * 100\n",
    "    high_missing = missing_pct[missing_pct > 90].index.tolist()\n",
    "    clean_df = clean_df.drop(columns=high_missing)\n",
    "\n",
    "\n",
    "    # High-correlation removal\n",
    "    num = clean_df.select_dtypes(include='number').columns\n",
    "    if len(num) > 1:\n",
    "        corr = clean_df[num].corr().abs()\n",
    "        to_drop = set()\n",
    "\n",
    "        missing = clean_df[num].isnull().mean()\n",
    "        var = clean_df[num].var()\n",
    "\n",
    "        for i, c1 in enumerate(num):\n",
    "            for j, c2 in enumerate(num):\n",
    "                if j <= i:\n",
    "                    continue\n",
    "                if corr.loc[c1, c2] > 0.95:\n",
    "                    if missing[c1] > missing[c2]:\n",
    "                        to_drop.add(c1)\n",
    "                    elif missing[c2] > missing[c1]:\n",
    "                        to_drop.add(c2)\n",
    "                    else:\n",
    "                        to_drop.add(c1 if var[c1] < var[c2] else c2)\n",
    "\n",
    "        clean_df = clean_df.drop(columns=list(to_drop))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Extreme-value capping\n",
    "    cols_to_cap_extreme = [\n",
    "    \"annual_inc\",\n",
    "    \"revol_bal\",\n",
    "    \"open_il_12m\",\n",
    "    \"mo_sin_old_il_acct\",\n",
    "    \"dti\",\n",
    "    \"mo_sin_old_rev_tl_op\"\n",
    "]\n",
    "\n",
    "    for column in cols_to_cap_extreme:\n",
    "        cap_value = clean_df[column].quantile(0.99999)\n",
    "        clean_df.loc[clean_df[column] > cap_value, column] = cap_value\n",
    "\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def minimal_clean2(df):\n",
    "    \"\"\"\n",
    "    Additional minimal cleaning:\n",
    "    Convert term and emp_length to numeric, extract issue year.\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    clean_df['term'] = clean_df['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    emp = clean_df['emp_length'].astype(str)\n",
    "    emp = emp.str.replace('< 1', '0', regex=False)\n",
    "    clean_df['emp_length'] = emp.str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    clean_df['issue_year'] = clean_df['issue_d'].astype(str).str.extract(r'(\\d{4})')[0].astype(float)\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def minimal_clean(df):\n",
    "    \"\"\"\n",
    "    Applies minimal_clean1 → minimal_clean2\n",
    "    \"\"\"\n",
    "    df1 = minimal_clean1(df)\n",
    "    df2 = minimal_clean2(df1)\n",
    "    return df2[df2[\"target_3class\"].notna()].reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "0c4fda86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:13:34.778178Z",
     "start_time": "2025-12-24T20:13:34.768546Z"
    }
   },
   "source": [
    "def make_preprocess(df):\n",
    "    \"\"\"\n",
    "    Builds preprocessing:\n",
    "      - Time since event columns (mths_since_*):\n",
    "          * Impute missing with 0 (interpreted as 'no event')\n",
    "          * Add missing indicator\n",
    "          * Scale\n",
    "      - Other numeric columns:\n",
    "          * Impute missing with median\n",
    "          * Add missing indicator\n",
    "          * Scale\n",
    "      - Categorical columns:\n",
    "          * Impute missing with string 'missing'\n",
    "          * One-hot encode (missing becomes category)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Identify column groups\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    time_cols = [col for col in numeric_cols if col.startswith(\"mths_since_\")]\n",
    "    num_regular = list(set(numeric_cols) - set(time_cols))\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "    # 2. Pipelines\n",
    "    time_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0, add_indicator=True)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    numeric_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    # 3. Combine all\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"time\", time_pipeline, time_cols),\n",
    "            (\"num\", numeric_pipeline, num_regular),\n",
    "            (\"cat\", categorical_pipeline, categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    return transformer"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "7319fc89de0ec9e8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-24T20:13:35.438864Z",
     "start_time": "2025-12-24T20:13:35.431516Z"
    }
   },
   "source": [
    "def build_model_pipeline(preprocess):\n",
    "    \"\"\"\n",
    "    Combines preprocessing + RandomForest classifier into a single Pipeline.\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "\n",
    "    return pipe"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "39f563a01f88e506",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def run_experiment(df, title, model_type):\n",
    "    \"\"\"\n",
    "    Runs training + test split + preprocessing + model training.\n",
    "    model_type: \"random_forest\", \"logistic\", \"xgboost\"\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Running: {title} ({model_type}) ===\")\n",
    "\n",
    "    # Split into features and target\n",
    "    X = df.drop(columns=[\"target_3class\"])\n",
    "    y = df[\"target_3class\"]\n",
    "    \n",
    "        # Convert target to numeric for XGBoost\n",
    "    if model_type == \"xgboost\":\n",
    "        y = y.astype(\"category\").cat.codes\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        stratify=y,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Build preprocessing based only on the feature matrix\n",
    "    preprocess = make_preprocess(X_train)\n",
    "\n",
    "    # Choose model\n",
    "    if model_type == \"random_forest\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=650,\n",
    "            max_depth=None,\n",
    "            min_samples_split=7,\n",
    "            min_samples_leaf=1,\n",
    "            max_features=0.5,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_type == \"xgboost\":\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=550,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=8,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            min_child_weight=1,\n",
    "            gamma=0.6,\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\"   # FAST + GPU compatible\n",
    "        )\n",
    "\n",
    "    elif model_type == \"logistic\":\n",
    "        model = LogisticRegression(\n",
    "            penalty='l2',\n",
    "            max_iter=2000,\n",
    "            l1_ratio=0.8,\n",
    "            class_weight=\"balanced\",\n",
    "            C=0.05\n",
    "        )\n",
    "\n",
    "    # Create pipeline\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", preprocess),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Fit\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # ---------- Evaluation ----------\n",
    "    # תחזיות על קבוצת ה-test\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # 1. Accuracy – אחוז הדוגמאות שנחזו נכון\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # 2. Macro F1 – ממוצע F1 לכל המחלקות\n",
    "    macro_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Macro F1:\", macro_f1)\n",
    "\n",
    "    # 3. Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # 4. Classification Report – Precision / Recall / F1 לכל מחלקה\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return acc, pipe"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cfd9efa3e2ed2849",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# =============================================================\n",
    "# Run 3 models on both Full and Minimal datasets\n",
    "# =============================================================\n",
    "df_with_target = create_target(df_raw)\n",
    "\n",
    "df_sample = df_with_target.sample(100000, random_state=42)\n",
    "\n",
    "df_full_sample = full_clean(df_sample.copy())\n",
    "df_minimal_sample = minimal_clean(df_sample.copy())\n",
    "\n",
    "\n",
    "models = [\"random_forest\", \"xgboost\", \"logistic\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(\"\\n=============================================================\")\n",
    "    print(f\"MODEL: {model_name.upper()}\")\n",
    "    print(\"=============================================================\")\n",
    "\n",
    "    # ---- FULL CLEAN ----\n",
    "    acc_full, model_full = run_experiment(\n",
    "        df_full_sample,\n",
    "        f\"Full Clean Sample - {model_name}\",\n",
    "        model_type=model_name\n",
    "    )\n",
    "\n",
    "    # ---- MINIMAL CLEAN ----\n",
    "    acc_minimal, model_minimal = run_experiment(\n",
    "        df_minimal_sample,\n",
    "        f\"Minimal Clean Sample - {model_name}\",\n",
    "        model_type=model_name\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    results[(model_name, \"full\")] = acc_full\n",
    "    results[(model_name, \"minimal\")] = acc_minimal\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# Print Summary Table\n",
    "# =============================================================\n",
    "print(\"\\n\\n==================== SUMMARY ====================\\n\")\n",
    "print(\"{:<20} {:<15} {:<15}\".format(\"Model\", \"Full Clean\", \"Minimal Clean\"))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name in models:\n",
    "    print(\"{:<20} {:<15.4f} {:<15.4f}\".format(\n",
    "        model_name,\n",
    "        results[(model_name, \"full\")],\n",
    "        results[(model_name, \"minimal\")]\n",
    "    ))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### handling outliers\n",
   "id": "2075f7ec7f7aed0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "full_df = create_target(df_raw)\n",
    "minimal_clean_full_df = minimal_clean(full_df)"
   ],
   "id": "763c487bf32d3d35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for col in minimal_clean_full_df.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.title(col)\n",
    "\n",
    "    # Numeric columns\n",
    "    if pd.api.types.is_numeric_dtype(minimal_clean_full_df[col]):\n",
    "        sns.histplot(minimal_clean_full_df[col].dropna(), kde=True)\n",
    "        plt.xlabel(col)\n",
    "\n",
    "        # Categorical or text columns\n",
    "    else:\n",
    "        num_unique = minimal_clean_full_df[col].nunique()\n",
    "\n",
    "        if num_unique <= 15:\n",
    "            minimal_clean_full_df[col].value_counts().plot(kind=\"bar\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Count\")\n",
    "        else:\n",
    "            print(f\"Skipping '{col}' (too many categories: {num_unique})\")\n",
    "            plt.close()\n",
    "            continue\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "a53e2d5a0e2310f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create the boxplot\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=minimal_clean_full_df['annual_inc'])"
   ],
   "id": "23de18b874816eb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create the boxplot\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=minimal_clean_full_df['dti'])"
   ],
   "id": "8a9376de63a550bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# לראות כמה ערכים יש עם DTI מעל 100%\n",
    "minimal_clean_full_df[minimal_clean_full_df['dti'] > 100].shape[0]"
   ],
   "id": "cc034567fedbed60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create the boxplot\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=minimal_clean_full_df['open_acc'])"
   ],
   "id": "6fc56a24f6a6c604",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create the boxplot\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=minimal_clean_full_df['pub_rec'])\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "id": "e66b4fcd1abf569c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=minimal_clean_full_df['revol_util'])\n",
    "plt.show()\n",
    "\n",
    "# לראות כמה ערכים יש עם  מעל 100%\n",
    "minimal_clean_full_df[minimal_clean_full_df['revol_util'] > 100].shape[0]"
   ],
   "id": "de7702f99b4398ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create the boxplot\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=minimal_clean_full_df['open_il_12m'])"
   ],
   "id": "b21c48b87379fcd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=minimal_clean_full_df['mo_sin_old_il_acct'])"
   ],
   "id": "90d2fe4684c977e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d7e499b3c3805c2",
   "metadata": {},
   "source": [
    "### parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "efeae3c45b4c87ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T15:10:38.125554Z",
     "start_time": "2025-12-23T15:10:22.093798Z"
    }
   },
   "source": [
    "#checking the number of values from each column:\n",
    "df_with_target = create_target(df_raw)\n",
    "df_with_target[\"target_3class\"].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_3class\n",
       "paid_on_time    1788541\n",
       "not_paid         261686\n",
       "paid_late        210441\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "c6a364017053cb9e",
   "metadata": {},
   "source": [
    "--- random forest ---"
   ]
  },
  {
   "cell_type": "code",
   "id": "f6324b1eb16cb65f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:15:15.606465Z",
     "start_time": "2025-12-24T20:14:24.602105Z"
    }
   },
   "source": [
    "# 1. Load data\n",
    "df_tuning = df.copy()\n",
    "\n",
    "# 2. Create target\n",
    "df_with_target_tuning = create_target(df_tuning)\n",
    "\n",
    "# 3. Choose minimal clean\n",
    "df_clean_tuning = minimal_clean(df_with_target_tuning)\n",
    "\n",
    "# 4. Make balanced N-row dataset\n",
    "def make_balanced_sample(df, N, target_col=\"target_3class\", random_state=42):\n",
    "    k = df[target_col].nunique()\n",
    "    per_class = N // k\n",
    "\n",
    "    balanced = (\n",
    "        df.groupby(target_col)\n",
    "          .apply(lambda g: g.sample(per_class, random_state=random_state))\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    remaining = N - len(balanced)\n",
    "    if remaining > 0:\n",
    "        extra = df.sample(remaining, random_state=random_state)\n",
    "        balanced = pd.concat([balanced, extra], ignore_index=True)\n",
    "\n",
    "    return balanced\n",
    "\n",
    "df_balanced_tuning = make_balanced_sample(df_clean_tuning, 20000)\n",
    "\n",
    "# 5. Split into X and y\n",
    "X = df_balanced_tuning.drop(columns=[\"target_3class\"])\n",
    "y = df_balanced_tuning[\"target_3class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6. Build preprocessing on X_train only\n",
    "preprocess_tuning = make_preprocess(X_train)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tamar\\AppData\\Local\\Temp\\ipykernel_19020\\308865155.py:71: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1233073.8432078396' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  clean_df.loc[clean_df[column] > cap_value, column] = cap_value\n",
      "C:\\Users\\tamar\\AppData\\Local\\Temp\\ipykernel_19020\\132350198.py:17: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(per_class, random_state=random_state))\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:36:24.123612Z",
     "start_time": "2025-12-24T15:36:24.119562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    \"clf__n_estimators\": [200, 700, 2000]\n",
    "    #1000 stops it completly\n",
    "   # \"model__max_depth\": [None, 10, 20],\n",
    "   # \"model__min_samples_leaf\": [1, 2, 5, 10],\n",
    "   # \"model__max_features\": [\"sqrt\", 0.3, 0.5],\n",
    "}"
   ],
   "id": "ef5e29cf84a3da2a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:36:26.626997Z",
     "start_time": "2025-12-24T15:36:26.616723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocess_tuning),\n",
    "    (\"clf\", RandomForestClassifier(random_state=42, n_jobs=1))\n",
    "])"
   ],
   "id": "f993082659884142",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-24T15:36:28.037277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=rf_pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ],
   "id": "6f65bddbbfd1c072",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e3e98e68364915c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e787240e1422cee0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9c4ef810a7064b51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b221e9ff6036ab1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T15:57:00.265758Z",
     "start_time": "2025-12-23T15:45:38.772374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocess_tuning),\n",
    "    (\"clf\", RandomForestClassifier(random_state=42, n_jobs=1))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"clf__n_estimators\": [500],\n",
    "    #\"clf__max_depth\": [None, 10, 20, 30],\n",
    "    #\"clf__max_features\": [\"sqrt\", 0.5],\n",
    "    #\"clf__min_samples_leaf\": [1, 5, 20],\n",
    "    #\"clf__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    rf_pipe,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print(\"RF best:\", rf_grid.best_score_)\n",
    "print(\"RF params:\", rf_grid.best_params_)\n"
   ],
   "id": "d4e18e06e14307ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 29\u001B[39m\n\u001B[32m     12\u001B[39m rf_param_grid = {\n\u001B[32m     13\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mclf__n_estimators\u001B[39m\u001B[33m\"\u001B[39m: [\u001B[32m500\u001B[39m],\n\u001B[32m     14\u001B[39m     \u001B[38;5;66;03m#\"clf__max_depth\": [None, 10, 20, 30],\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     17\u001B[39m     \u001B[38;5;66;03m#\"clf__class_weight\": [None, \"balanced\"],\u001B[39;00m\n\u001B[32m     18\u001B[39m }\n\u001B[32m     20\u001B[39m rf_grid = GridSearchCV(\n\u001B[32m     21\u001B[39m     rf_pipe,\n\u001B[32m     22\u001B[39m     param_grid=rf_param_grid,\n\u001B[32m   (...)\u001B[39m\u001B[32m     26\u001B[39m     verbose=\u001B[32m2\u001B[39m\n\u001B[32m     27\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m29\u001B[39m \u001B[43mrf_grid\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mRF best:\u001B[39m\u001B[33m\"\u001B[39m, rf_grid.best_score_)\n\u001B[32m     31\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mRF params:\u001B[39m\u001B[33m\"\u001B[39m, rf_grid.best_params_)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001B[39m, in \u001B[36mBaseSearchCV.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m   1045\u001B[39m     results = \u001B[38;5;28mself\u001B[39m._format_results(\n\u001B[32m   1046\u001B[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[32m   1047\u001B[39m     )\n\u001B[32m   1049\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m-> \u001B[39m\u001B[32m1051\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[32m   1054\u001B[39m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[32m   1055\u001B[39m first_test_score = all_out[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtest_scores\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001B[39m, in \u001B[36mGridSearchCV._run_search\u001B[39m\u001B[34m(self, evaluate_candidates)\u001B[39m\n\u001B[32m   1603\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[32m   1604\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1605\u001B[39m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001B[39m, in \u001B[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[39m\u001B[34m(candidate_params, cv, more_results)\u001B[39m\n\u001B[32m    989\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose > \u001B[32m0\u001B[39m:\n\u001B[32m    990\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    991\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m candidates,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    992\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m fits\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    993\u001B[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001B[32m    994\u001B[39m         )\n\u001B[32m    995\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m997\u001B[39m out = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    998\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    999\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1000\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1001\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1002\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1003\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1004\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1005\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1006\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1007\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1008\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1015\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) < \u001B[32m1\u001B[39m:\n\u001B[32m   1016\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1017\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo fits were performed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1018\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWas the CV iterator empty? \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1019\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWere there no candidates?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1020\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     73\u001B[39m warning_filters = warnings.filters\n\u001B[32m     74\u001B[39m iterable_with_config_and_warning_filters = (\n\u001B[32m     75\u001B[39m     (\n\u001B[32m     76\u001B[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001B[32m   (...)\u001B[39m\u001B[32m     80\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     81\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config_and_warning_filters\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:2072\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   2066\u001B[39m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[32m   2067\u001B[39m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[32m   2068\u001B[39m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[32m   2069\u001B[39m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[32m   2070\u001B[39m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m2072\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1682\u001B[39m, in \u001B[36mParallel._get_outputs\u001B[39m\u001B[34m(self, iterator, pre_dispatch)\u001B[39m\n\u001B[32m   1679\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m   1681\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.retrieval_context():\n\u001B[32m-> \u001B[39m\u001B[32m1682\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retrieve()\n\u001B[32m   1684\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[32m   1685\u001B[39m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[32m   1686\u001B[39m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[32m   1687\u001B[39m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[32m   1688\u001B[39m     \u001B[38;5;28mself\u001B[39m._exception = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1800\u001B[39m, in \u001B[36mParallel._retrieve\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1789\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_ordered:\n\u001B[32m   1790\u001B[39m     \u001B[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001B[39;00m\n\u001B[32m   1791\u001B[39m     \u001B[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1795\u001B[39m     \u001B[38;5;66;03m# control only have to be done on the amount of time the next\u001B[39;00m\n\u001B[32m   1796\u001B[39m     \u001B[38;5;66;03m# dispatched job is pending.\u001B[39;00m\n\u001B[32m   1797\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (nb_jobs == \u001B[32m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   1798\u001B[39m         \u001B[38;5;28mself\u001B[39m._jobs[\u001B[32m0\u001B[39m].get_status(timeout=\u001B[38;5;28mself\u001B[39m.timeout) == TASK_PENDING\n\u001B[32m   1799\u001B[39m     ):\n\u001B[32m-> \u001B[39m\u001B[32m1800\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1801\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1803\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m nb_jobs == \u001B[32m0\u001B[39m:\n\u001B[32m   1804\u001B[39m     \u001B[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001B[39;00m\n\u001B[32m   1805\u001B[39m     \u001B[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1811\u001B[39m     \u001B[38;5;66;03m# timeouts before any other dispatched job has completed and\u001B[39;00m\n\u001B[32m   1812\u001B[39m     \u001B[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "1be4a7761ac4c3c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T14:44:19.807165Z",
     "start_time": "2025-12-24T14:44:16.363034Z"
    }
   },
   "source": [
    "rf_pipe_tuning = Pipeline([\n",
    "    (\"pre\", preprocess_tuning),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_pipe_tuning.fit(X_train, y_train)\n",
    "y_pred = rf_pipe_tuning.predict(X_test)\n",
    "\n",
    "baseline_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"Baseline Macro F1:\", baseline_f1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Macro F1: 0.6790028016193377\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "f7ecb260c0d0eccc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T15:26:07.379101Z",
     "start_time": "2025-12-23T15:26:07.360986Z"
    }
   },
   "source": [
    "rf_params_coarse = {\n",
    "    # We tune in phases so the computer can handle it. After deciding on each phase, moving on to the next one.\n",
    "\n",
    "    # first:\n",
    "    #\"clf__max_depth\": [None, 3, 5, 8, 12, 16, 24, 32, 40],\n",
    "    \"clf__max_depth\": [None, 24]\n",
    "    #\"clf__min_samples_leaf\": [1, 16],\n",
    "    #\"clf__n_estimators\": [100, 600],\n",
    "\n",
    "    # second\n",
    "    #\"clf__max_features\": [\"sqrt\", \"log2\", 0.7, 1.0],\n",
    "    #\"clf__max_features\": [0.7],\n",
    "\n",
    "\n",
    "    #third\n",
    "    #\"clf__min_samples_split\": [2, 50],\n",
    "    #\"clf__min_samples_split\": [20],\n",
    "    #\"clf__class_weight\": [None, \"balanced\"]\n",
    "\n",
    "}\n",
    "\n",
    "rf_param_dist = {\n",
    "    \"n_estimators\": [300, 600, 1000, 1500, 2000],\n",
    "    \"max_depth\": [None, 3, 5, 8, 12, 16, 24, 32, 40],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    \"min_samples_split\": [2, 5, 10, 20, 30, 50],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 8, 16, 32, 50],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"max_samples\": [0.5, 0.7, 0.9, 1.0],\n",
    "    \"class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "}\n",
    "\n",
    "\n",
    "fixed = {\n",
    "    'clf__class_weight' : ['balanced'],\n",
    "    \"clf__max_features\": [0.7]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "4f8b6722955f4073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T15:29:25.260600Z",
     "start_time": "2025-12-23T15:29:23.874435Z"
    }
   },
   "source": [
    "rf_pipe = Pipeline([\n",
    "    (\"pre\", preprocess_tuning),\n",
    "    (\"clf\", RandomForestClassifier(max_features=0.7, class_weight = 'balanced', random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "rf_search_coarse = RandomizedSearchCV(\n",
    "    estimator=rf_pipe,\n",
    "    param_distributions=rf_params_coarse,\n",
    "    n_iter=1,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_search_coarse.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m      1\u001B[39m rf_pipe = Pipeline([\n\u001B[32m      2\u001B[39m     (\u001B[33m\"\u001B[39m\u001B[33mpre\u001B[39m\u001B[33m\"\u001B[39m, preprocess_tuning),\n\u001B[32m      3\u001B[39m     (\u001B[33m\"\u001B[39m\u001B[33mclf\u001B[39m\u001B[33m\"\u001B[39m, RandomForestClassifier(max_features=\u001B[32m0.7\u001B[39m, class_weight = \u001B[33m'\u001B[39m\u001B[33mbalanced\u001B[39m\u001B[33m'\u001B[39m, random_state=\u001B[32m42\u001B[39m, n_jobs=-\u001B[32m1\u001B[39m))\n\u001B[32m      4\u001B[39m ])\n\u001B[32m      6\u001B[39m rf_search_coarse = RandomizedSearchCV(\n\u001B[32m      7\u001B[39m     estimator=rf_pipe,\n\u001B[32m      8\u001B[39m     param_distributions=rf_params_coarse,\n\u001B[32m   (...)\u001B[39m\u001B[32m     14\u001B[39m     random_state=\u001B[32m42\u001B[39m\n\u001B[32m     15\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m \u001B[43mrf_search_coarse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001B[39m, in \u001B[36mBaseSearchCV.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m   1045\u001B[39m     results = \u001B[38;5;28mself\u001B[39m._format_results(\n\u001B[32m   1046\u001B[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[32m   1047\u001B[39m     )\n\u001B[32m   1049\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m-> \u001B[39m\u001B[32m1051\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[32m   1054\u001B[39m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[32m   1055\u001B[39m first_test_score = all_out[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtest_scores\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1992\u001B[39m, in \u001B[36mRandomizedSearchCV._run_search\u001B[39m\u001B[34m(self, evaluate_candidates)\u001B[39m\n\u001B[32m   1990\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[32m   1991\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1992\u001B[39m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1993\u001B[39m \u001B[43m        \u001B[49m\u001B[43mParameterSampler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1994\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparam_distributions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrandom_state\u001B[49m\n\u001B[32m   1995\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1996\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001B[39m, in \u001B[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[39m\u001B[34m(candidate_params, cv, more_results)\u001B[39m\n\u001B[32m    989\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose > \u001B[32m0\u001B[39m:\n\u001B[32m    990\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    991\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m candidates,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    992\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m fits\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    993\u001B[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001B[32m    994\u001B[39m         )\n\u001B[32m    995\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m997\u001B[39m out = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    998\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    999\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1000\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1001\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1002\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1003\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1004\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1005\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1006\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1007\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1008\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1015\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) < \u001B[32m1\u001B[39m:\n\u001B[32m   1016\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1017\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo fits were performed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1018\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWas the CV iterator empty? \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1019\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWere there no candidates?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1020\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     73\u001B[39m warning_filters = warnings.filters\n\u001B[32m     74\u001B[39m iterable_with_config_and_warning_filters = (\n\u001B[32m     75\u001B[39m     (\n\u001B[32m     76\u001B[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001B[32m   (...)\u001B[39m\u001B[32m     80\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     81\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config_and_warning_filters\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:2072\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   2066\u001B[39m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[32m   2067\u001B[39m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[32m   2068\u001B[39m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[32m   2069\u001B[39m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[32m   2070\u001B[39m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m2072\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1682\u001B[39m, in \u001B[36mParallel._get_outputs\u001B[39m\u001B[34m(self, iterator, pre_dispatch)\u001B[39m\n\u001B[32m   1679\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m   1681\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.retrieval_context():\n\u001B[32m-> \u001B[39m\u001B[32m1682\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retrieve()\n\u001B[32m   1684\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[32m   1685\u001B[39m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[32m   1686\u001B[39m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[32m   1687\u001B[39m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[32m   1688\u001B[39m     \u001B[38;5;28mself\u001B[39m._exception = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1800\u001B[39m, in \u001B[36mParallel._retrieve\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1789\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_ordered:\n\u001B[32m   1790\u001B[39m     \u001B[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001B[39;00m\n\u001B[32m   1791\u001B[39m     \u001B[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1795\u001B[39m     \u001B[38;5;66;03m# control only have to be done on the amount of time the next\u001B[39;00m\n\u001B[32m   1796\u001B[39m     \u001B[38;5;66;03m# dispatched job is pending.\u001B[39;00m\n\u001B[32m   1797\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (nb_jobs == \u001B[32m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   1798\u001B[39m         \u001B[38;5;28mself\u001B[39m._jobs[\u001B[32m0\u001B[39m].get_status(timeout=\u001B[38;5;28mself\u001B[39m.timeout) == TASK_PENDING\n\u001B[32m   1799\u001B[39m     ):\n\u001B[32m-> \u001B[39m\u001B[32m1800\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1801\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1803\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m nb_jobs == \u001B[32m0\u001B[39m:\n\u001B[32m   1804\u001B[39m     \u001B[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001B[39;00m\n\u001B[32m   1805\u001B[39m     \u001B[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1811\u001B[39m     \u001B[38;5;66;03m# timeouts before any other dispatched job has completed and\u001B[39;00m\n\u001B[32m   1812\u001B[39m     \u001B[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "b7d8a7e370025838",
   "metadata": {},
   "source": [
    "print(\"Best coarse params:\")\n",
    "print(rf_search_coarse.best_params_)\n",
    "\n",
    "print(\"Best CV f1_macro:\")\n",
    "print(rf_search_coarse.best_score_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "55afdb28491a60f7",
   "metadata": {},
   "source": [
    "--- xg boost ---"
   ]
  },
  {
   "cell_type": "code",
   "id": "7908434165118e4c",
   "metadata": {},
   "source": [
    "y_train = y_train.astype(\"category\").cat.codes\n",
    "y_test = y_test.astype(\"category\").cat.codes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "55a5dce62d9f53f9",
   "metadata": {},
   "source": [
    "xgb_pipe_tuning = Pipeline([\n",
    "    (\"pre\", preprocess_tuning),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=y.nunique(),\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_pipe_tuning.fit(X_train, y_train)\n",
    "y_pred = xgb_pipe_tuning.predict(X_test)\n",
    "baseline_xgb_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"Baseline XGB Macro F1:\", baseline_xgb_f1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "165acb9683dddd04",
   "metadata": {},
   "source": [
    "xgb_params_coarse = {\n",
    "# We tune in phases so the computer can handle it. After deciding on each phase, moving on to the next one.\n",
    "\n",
    "    #first\n",
    "    \"clf__n_estimators\":        [200, 400, 800, 1200],\n",
    "    \"clf__learning_rate\":       [0.01, 0.05, 0.1],\n",
    "    \"clf__max_depth\":           [3, 7, 10, None],\n",
    "\n",
    "    #second\n",
    "    \"clf__min_child_weight\":    [1, 3, 5],\n",
    "    \"clf__subsample\":           [0.6, 0.8, 1.0],\n",
    "    \"clf__colsample_bytree\":    [0.6, 0.8, 1.0],\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8937b31d48905be5",
   "metadata": {},
   "source": [
    "xgb_search_coarse = RandomizedSearchCV(\n",
    "    estimator=xgb_pipe_tuning,\n",
    "    param_distributions=xgb_params_coarse,\n",
    "    n_iter=40,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_search_coarse.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best coarse params (XGB):\")\n",
    "print(xgb_search_coarse.best_params_)\n",
    "print(\"Best CV f1_macro (XGB):\")\n",
    "print(xgb_search_coarse.best_score_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b5abbecab4f9bf5a",
   "metadata": {},
   "source": [
    "--- logistic regression ---"
   ]
  },
  {
   "cell_type": "code",
   "id": "8404e46fe46b72f1",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_pipe_tuning = Pipeline([\n",
    "    (\"pre\", preprocess_tuning),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "lr_pipe_tuning.fit(X_train, y_train)\n",
    "y_pred = lr_pipe_tuning.predict(X_test)\n",
    "baseline_lr_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"Baseline LR Macro F1:\", baseline_lr_f1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "603cc8a4703b9745",
   "metadata": {},
   "source": [
    "lr_params_coarse = {\n",
    "# We tune in phases so the computer can handle it. After deciding on each phase, moving on to the next one.\n",
    "\n",
    "    #first\n",
    "    \"clf__C\":             [1e-3, 1e-2, 1e-1, 1, 5, 10],\n",
    "    \"clf__solver\":       [\"lbfgs\", \"saga\"],\n",
    "    \"clf__penalty\":      [\"l2\", \"elasticnet\"],\n",
    "\n",
    "    #second\n",
    "    \"clf__l1_ratio\":     [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "    \"clf__max_iter\":     [1000, 2000, 4000],\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8fd501ebd113b12f",
   "metadata": {},
   "source": [
    "lr_search_coarse = RandomizedSearchCV(\n",
    "    estimator=lr_pipe_tuning,\n",
    "    param_distributions=lr_params_coarse,\n",
    "    n_iter=40,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lr_search_coarse.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best coarse params (LR):\")\n",
    "print(lr_search_coarse.best_params_)\n",
    "print(\"Best CV f1_macro (LR):\")\n",
    "print(lr_search_coarse.best_score_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3acb1a392c5529fa",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1ad86636780c61d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4df21b57088a63bb",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "117827833b07491f",
   "metadata": {},
   "source": [
    "df_full_sample[\"target_3class\"].value_counts()\n",
    "df_full_sample[\"target_3class\"].value_counts(normalize=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a86cb57b0bd053e",
   "metadata": {},
   "source": [
    "df_minimal_sample[\"target_3class\"].value_counts()\n",
    "df_minimal_sample[\"target_3class\"].value_counts(normalize=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "82530d3d863ee6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4c8922347edc1e9"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-24T20:19:22.255332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Copy/paste standard NN (MLPClassifier) pipeline that AUTO-detects numeric vs categorical columns.\n",
    "# You only need to set: target_col and df_clean.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) INPUTS YOU SET\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "# 1. Load data\n",
    "df_tuning = df.copy()\n",
    "\n",
    "# 2. Create target\n",
    "df_with_target_tuning = create_target(df_tuning)\n",
    "\n",
    "# 3. Choose minimal clean\n",
    "df_clean_tuning = minimal_clean(df_with_target_tuning)\n",
    "\n",
    "\n",
    "df_model = df_clean_tuning.copy()       # <-- your cleaned dataframe\n",
    "target_col = \"target_3class\"     # <-- your target column\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) SPLIT X / y\n",
    "# -----------------------------\n",
    "X = df_model.drop(columns=[target_col])\n",
    "y = df_model[target_col]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) AUTO-DETECT COLUMN TYPES\n",
    "# -----------------------------\n",
    "# Numeric: numbers + booleans\n",
    "numeric_features = X.select_dtypes(include=[np.number, \"bool\"]).columns.tolist()\n",
    "\n",
    "# Datetime: we'll DROP by default (you can engineer features later if needed)\n",
    "datetime_features = X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns.tolist()\n",
    "\n",
    "# Categorical: objects + pandas 'category' (excluding datetime)\n",
    "categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# Safety: remove datetime from X so it doesn't cause transformer issues\n",
    "if datetime_features:\n",
    "    X = X.drop(columns=datetime_features)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) TRAIN/TEST SPLIT\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y if y.nunique() > 1 else None\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) PREPROCESSING\n",
    "# -----------------------------\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, numeric_features),\n",
    "        (\"cat\", categorical_pipe, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) MODEL: BASIC NEURAL NET\n",
    "# -----------------------------\n",
    "pipe_nn = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-4,\n",
    "        learning_rate_init=1e-3,\n",
    "        batch_size=256,\n",
    "        max_iter=50,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) FIT + EVALUATE\n",
    "# -----------------------------\n",
    "pipe_nn.fit(X_train, y_train)\n",
    "\n",
    "pred = pipe_nn.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "if hasattr(pipe_nn, \"predict_proba\"):\n",
    "    proba = pipe_nn.predict_proba(X_test)\n",
    "    if y.nunique() == 2:\n",
    "        auc = roc_auc_score(y_test, proba[:, 1])\n",
    "        print(f\"ROC-AUC: {auc:.4f}\")\n",
    "    else:\n",
    "        auc_ovr = roc_auc_score(y_test, proba, multi_class=\"ovr\")\n",
    "        print(f\"ROC-AUC (OVR): {auc_ovr:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Optional: see what it auto-detected\n",
    "print(f\"Detected numeric cols: {len(numeric_features)}\")\n",
    "print(f\"Detected categorical cols: {len(categorical_features)}\")\n",
    "print(f\"Dropped datetime cols: {len(datetime_features)} -> {datetime_features[:5]}\")"
   ],
   "id": "9febc56eecd590ba",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tamar\\AppData\\Local\\Temp\\ipykernel_19020\\308865155.py:71: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1233073.8432078396' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  clean_df.loc[clean_df[column] > cap_value, column] = cap_value\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d935d839c1efbe4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
