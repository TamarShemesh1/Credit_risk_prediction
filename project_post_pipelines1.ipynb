{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:18:28.214229300Z",
     "start_time": "2025-11-29T18:18:15.018801700Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6c5c202d42c147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:19:13.715873100Z",
     "start_time": "2025-11-29T18:18:28.216458100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wn/wpy04pb15y934q92j5gbcvb80000gn/T/ipykernel_11548/3977509200.py:2: DtypeWarning: Columns (19,47,55,112,123,124,125,128,129,130,133,139,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"loan.csv\")\n"
     ]
    }
   ],
   "source": [
    "# loading the data\n",
    "df = pd.read_csv(\"loan.csv\")\n",
    "df.shape\n",
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3812168bef44e6a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:19:13.766876200Z",
     "start_time": "2025-11-29T18:19:13.746193800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Step 1: Create the 3-class target (common for both pipelines)\n",
    "# -------------------------------------------------------------\n",
    "def create_target(df):\n",
    "    \"\"\"\n",
    "    Creates the 3-class target:\n",
    "      - paid_on_time\n",
    "      - paid_late\n",
    "      - not_paid\n",
    "\n",
    "    Also computes helper columns needed to classify paid_late correctly.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%Y', errors='coerce')\n",
    "    df['last_pymnt_d'] = pd.to_datetime(df['last_pymnt_d'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "    # Extract loan term in months\n",
    "    df['term_months'] = df['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    # Approximate expected end date\n",
    "    df['expected_end'] = df['issue_d'] + pd.to_timedelta(df['term_months'] * 30, unit='D')\n",
    "\n",
    "    # Late fully paid flag\n",
    "    df['paid_late_flag'] = (\n",
    "        (df['loan_status'] == 'Fully Paid') &\n",
    "        (df['last_pymnt_d'] > df['expected_end'])\n",
    "    )\n",
    "\n",
    "    # Build target variable\n",
    "    df['target_3class'] = 'paid_on_time'\n",
    "    df.loc[df['paid_late_flag'], 'target_3class'] = 'paid_late'\n",
    "    df.loc[df['loan_status'].isin(['Charged Off', 'Default']), 'target_3class'] = 'not_paid'\n",
    "    # Remove '(future leakage + breaks categorical encoding)\n",
    "    df = df.drop(columns=['next_pymnt_d', 'paid_late_flag', 'last_pymnt_d'], errors='ignore')\n",
    "\n",
    "\n",
    "    return df[df['target_3class'].notna()].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651817f39d5a64fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:19:13.820332200Z",
     "start_time": "2025-11-29T18:19:13.787996600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# FULL CLEAN PIPELINE (30 variables clean)\n",
    "# =============================================================\n",
    "leakage_columns = [\n",
    "    'hardship_flag','debt_settlement_flag','total_pymnt','total_rec_prncp',\n",
    "    'total_rec_int','total_rec_late_fee','last_pymnt_d','last_pymnt_amnt',\n",
    "    'recoveries','collection_recovery_fee','out_prncp','total_pymnt_inv',\n",
    "    'out_prncp_inv','loan_status'\n",
    "]\n",
    "\n",
    "high_corr_drop_columns = [\n",
    "    'funded_amnt','funded_amnt_inv','installment',\n",
    "    'num_rev_tl_bal_gt_0','tot_hi_cred_lim',\n",
    "    'total_il_high_credit_limit','num_sats'\n",
    "]\n",
    "\n",
    "redundant_columns = ['policy_code','disbursement_method','chargeoff_within_12_mths', 'initial_list_status']\n",
    "\n",
    "\n",
    "def basic_clean1(df):\n",
    "    \"\"\"\n",
    "    First stage of full clean:\n",
    "    - Remove leakage, high correlation, and redundant columns\n",
    "    - Drop columns with >90% missing\n",
    "    - Clean term and emp_length\n",
    "    - Remove rows with missing target\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    cols_to_drop = [c for c in (leakage_columns + high_corr_drop_columns + redundant_columns) \n",
    "                    if c in clean_df.columns]\n",
    "    clean_df = clean_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Drop columns with excessive missingness\n",
    "    missing_pct = clean_df.isnull().mean() * 100\n",
    "    high_missing_cols = missing_pct[missing_pct > 90].index.tolist()\n",
    "    clean_df = clean_df.drop(columns=high_missing_cols)\n",
    "\n",
    "    # Remove \"Not Verified\"\n",
    "    clean_df = clean_df[clean_df['verification_status'] != 'Not Verified']\n",
    "    clean_df = clean_df.drop(columns=['verification_status'], errors='ignore')\n",
    "\n",
    "    # Clean term (36/60)\n",
    "    clean_df['term'] = clean_df['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    # Clean employee length\n",
    "    emp = clean_df['emp_length'].astype(str)\n",
    "    emp = emp.str.replace('< 1', '0', regex=False)\n",
    "    clean_df['emp_length'] = emp.str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    return clean_df[clean_df['target_3class'].notna()].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def basic_clean2(df):\n",
    "    \"\"\"\n",
    "    Second stage of full clean:\n",
    "    - Remove non-informative columns\n",
    "    - Normalize text columns\n",
    "    - Create engineered features (purpose_grouped, home_stability, credit_age_years, etc.)\n",
    "    - Remove raw columns after feature engineering\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    # Non-informative\n",
    "    for col in ['pymnt_plan','zip_code','collections_12_mths_ex_med']:\n",
    "        clean_df = clean_df.drop(columns=col, errors='ignore')\n",
    "\n",
    "    # Lowercase text columns\n",
    "    for col in ['purpose','home_ownership','addr_state','application_type','emp_title']:\n",
    "        if col in clean_df:\n",
    "            clean_df[col] = clean_df[col].astype(str).str.lower().str.strip()\n",
    "\n",
    "    # Grouped purpose variable\n",
    "    clean_df['purpose_grouped'] = clean_df['purpose'].replace({\n",
    "        'debt_consolidation':'debt','credit_card':'debt',\n",
    "        'home_improvement':'housing','house':'housing',\n",
    "        'small_business':'business',\n",
    "        'car':'personal','medical':'personal','vacation':'personal','moving':'personal',\n",
    "        'wedding':'personal','major_purchase':'personal',\n",
    "        'renewable_energy':'other','educational':'other','other':'other'\n",
    "    })\n",
    "\n",
    "    # Home stability\n",
    "    clean_df['home_stability'] = clean_df['home_ownership'].replace({\n",
    "        'mortgage':'stable','own':'stable',\n",
    "        'rent':'unstable','none':'unstable','other':'unstable'\n",
    "    })\n",
    "\n",
    "    # Credit age\n",
    "    years = clean_df['earliest_cr_line'].astype(str).str.extract(r'(\\d{4})')[0].astype(float)\n",
    "    clean_df['credit_age_years'] = datetime.now().year - years\n",
    "\n",
    "    # Create engineered count-like fields\n",
    "    clean_df['bad_records_count'] = clean_df[['pub_rec','pub_rec_bankruptcies','tax_liens']].sum(axis=1, min_count=1)\n",
    "    clean_df['recent_credit_activity'] = (\n",
    "        clean_df['inq_last_6mths'] + clean_df['num_tl_op_past_12m'] - (clean_df['mths_since_recent_inq']/12)\n",
    "    )\n",
    "\n",
    "    clean_df['total_balance_all'] = clean_df['tot_cur_bal'] + clean_df['total_bal_il']\n",
    "    clean_df['active_credit_accounts'] = clean_df['num_actv_bc_tl'] + clean_df['num_actv_rev_tl']\n",
    "\n",
    "    # Binary delinquency flag\n",
    "    clean_df['any_delinquency'] = (\n",
    "        (clean_df['num_accts_ever_120_pd']>0) |\n",
    "        (clean_df['num_tl_120dpd_2m']>0) |\n",
    "        (clean_df['num_tl_90g_dpd_24m']>0) |\n",
    "        (clean_df['num_tl_30dpd']>0) |\n",
    "        (clean_df['delinq_2yrs']>0)\n",
    "    ).astype(int)\n",
    "\n",
    "    clean_df['is_joint_app'] = clean_df['application_type'].str.contains('joint').astype(int)\n",
    "    clean_df['has_current_delinquency'] = (clean_df['acc_now_delinq']>0).astype(int)\n",
    "    clean_df['has_collections'] = (clean_df['tot_coll_amt']>0).astype(int)\n",
    "\n",
    "    # Drop raw columns after creating engineered features\n",
    "    columns_to_remove = [\n",
    "        'purpose','home_ownership','earliest_cr_line','application_type',\n",
    "        'acc_now_delinq','tot_coll_amt','title','emp_title',\n",
    "        'addr_state','issue_d','last_credit_pull_d','loan_status',\n",
    "        'days_late','open_acc','revol_bal','pub_rec','pub_rec_bankruptcies',\n",
    "        'tax_liens','inq_last_6mths','num_tl_op_past_12m','mths_since_recent_inq',\n",
    "        'tot_cur_bal','total_bal_il','num_actv_bc_tl','num_actv_rev_tl',\n",
    "        'num_accts_ever_120_pd','num_tl_120dpd_2m','num_tl_90g_dpd_24m',\n",
    "        'delinq_2yrs','num_tl_30dpd'\n",
    "    ]\n",
    "\n",
    "    clean_df = clean_df.drop(columns=[c for c in columns_to_remove if c in clean_df.columns])\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def basic_clean3(df):\n",
    "    \"\"\"\n",
    "    Final stage of full clean:\n",
    "    Removes noisy, sparse, redundant, or low-importance features.\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    low_importance = [\n",
    "        'open_act_il','open_il_12m','open_il_24m','open_rv_12m','open_rv_24m',\n",
    "        'open_acc_6m','inq_fi','total_cu_tl','acc_open_past_24mths'\n",
    "    ]\n",
    "\n",
    "    redundant = [\n",
    "        'num_bc_sats','num_bc_tl','num_il_tl','num_op_rev_tl',\n",
    "        'num_rev_accts','total_acc'\n",
    "    ]\n",
    "\n",
    "    sparse = [\n",
    "        'mths_since_last_record','mths_since_recent_bc_dlq',\n",
    "        'mths_since_recent_revol_delinq','percent_bc_gt_75'\n",
    "    ]\n",
    "\n",
    "    noisy = [\n",
    "        'mo_sin_old_il_acct','mo_sin_old_rev_tl_op',\n",
    "        'mo_sin_rcnt_rev_tl_op','mo_sin_rcnt_tl',\n",
    "        'mths_since_rcnt_il'\n",
    "    ]\n",
    "\n",
    "    optional = [\n",
    "        'mths_since_last_delinq','avg_cur_bal','max_bal_bc','all_util','il_util',\n",
    "        'inq_last_12m','pct_tl_nvr_dlq','mort_acc','total_bc_limit','total_acc'\n",
    "    ]\n",
    "\n",
    "    to_drop = low_importance + redundant + sparse + noisy + optional\n",
    "    clean_df = clean_df.drop(columns=[c for c in to_drop if c in clean_df.columns])\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def full_clean(df):\n",
    "    \"\"\"\n",
    "    Applies the full 3-stage cleaning process:\n",
    "    basic_clean1 → basic_clean2 → basic_clean3\n",
    "    \"\"\"\n",
    "    df1 = basic_clean1(df)\n",
    "    df2 = basic_clean2(df1)\n",
    "    df3 = basic_clean3(df2)\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b82304b58741665d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:19:13.865832400Z",
     "start_time": "2025-11-29T18:19:13.820332200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# MINIMAL CLEAN PIPELINE\n",
    "# Extracted from: data minimal clean.py\n",
    "# =============================================================\n",
    "def minimal_clean1(df):\n",
    "    \"\"\"\n",
    "    Minimal clean:\n",
    "    - Remove leakage\n",
    "    - Remove columns with >90% missing\n",
    "    - Remove one feature from each high-correlation (>0.95) pair\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    leakage = [\n",
    "        'hardship_flag','debt_settlement_flag','total_pymnt','total_rec_prncp',\n",
    "        'total_rec_int','total_rec_late_fee','last_pymnt_d','last_pymnt_amnt',\n",
    "        'recoveries','collection_recovery_fee','out_prncp','total_pymnt_inv',\n",
    "        'out_prncp_inv','loan_status','paid_late_flag'\n",
    "    ]\n",
    "    clean_df = clean_df.drop(columns=[c for c in leakage if c in clean_df.columns])\n",
    "\n",
    "    # Drop >90% missing\n",
    "    missing_pct = clean_df.isnull().mean() * 100\n",
    "    high_missing = missing_pct[missing_pct > 90].index.tolist()\n",
    "    clean_df = clean_df.drop(columns=high_missing)\n",
    "\n",
    "    # High-correlation removal\n",
    "    num = clean_df.select_dtypes(include='number').columns\n",
    "    if len(num) > 1:\n",
    "        corr = clean_df[num].corr().abs()\n",
    "        to_drop = set()\n",
    "\n",
    "        missing = clean_df[num].isnull().mean()\n",
    "        var = clean_df[num].var()\n",
    "\n",
    "        for i, c1 in enumerate(num):\n",
    "            for j, c2 in enumerate(num):\n",
    "                if j <= i:\n",
    "                    continue\n",
    "                if corr.loc[c1, c2] > 0.95:\n",
    "                    if missing[c1] > missing[c2]:\n",
    "                        to_drop.add(c1)\n",
    "                    elif missing[c2] > missing[c1]:\n",
    "                        to_drop.add(c2)\n",
    "                    else:\n",
    "                        to_drop.add(c1 if var[c1] < var[c2] else c2)\n",
    "\n",
    "        clean_df = clean_df.drop(columns=list(to_drop))\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def minimal_clean2(df):\n",
    "    \"\"\"\n",
    "    Additional minimal cleaning:\n",
    "    Convert term and emp_length to numeric, extract issue year.\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    clean_df['term'] = clean_df['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    emp = clean_df['emp_length'].astype(str)\n",
    "    emp = emp.str.replace('< 1', '0', regex=False)\n",
    "    clean_df['emp_length'] = emp.str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    clean_df['issue_year'] = clean_df['issue_d'].astype(str).str.extract(r'(\\d{4})')[0].astype(float)\n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "def minimal_clean(df):\n",
    "    \"\"\"\n",
    "    Applies minimal_clean1 → minimal_clean2\n",
    "    \"\"\"\n",
    "    df1 = minimal_clean1(df)\n",
    "    df2 = minimal_clean2(df1)\n",
    "    return df2[df2[\"target_3class\"].notna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c4fda86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocess(df):\n",
    "    \"\"\"\n",
    "    Builds preprocessing:\n",
    "      - Time since event columns (mths_since_*):\n",
    "          * Impute missing with 0  (interpreted as 'no event')\n",
    "          * Add missing indicator\n",
    "          * Scale\n",
    "      - Other numeric columns:\n",
    "          * Impute missing with median\n",
    "          * Add missing indicator\n",
    "          * Scale\n",
    "      - Categorical columns (including ordinal-like text):\n",
    "          * Impute missing with string 'missing'\n",
    "          * One-hot encode (missing becomes its own category)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Identify column groups ---\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    time_cols = [col for col in numeric_cols if col.startswith(\"mths_since_\")]\n",
    "    num_regular = list(set(numeric_cols) - set(time_cols))\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "    # --- 2. Define pipelines for each group ---\n",
    "    time_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0, add_indicator=True)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    numeric_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    # --- 3. Combine all in ColumnTransformer ---\n",
    "\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"time\", time_pipeline, time_cols),\n",
    "            (\"num\", numeric_pipeline, num_regular),\n",
    "            (\"cat\", categorical_pipeline, categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\" \n",
    "    )\n",
    "\n",
    "    return transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7319fc89de0ec9e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:19:13.916246700Z",
     "start_time": "2025-11-29T18:19:13.898359800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model_pipeline(preprocess):\n",
    "    \"\"\"\n",
    "    Combines preprocessing + RandomForest classifier into a single Pipeline.\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39f563a01f88e506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:19:13.949840300Z",
     "start_time": "2025-11-29T18:19:13.922957Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_experiment(df, title, model_type):\n",
    "    \"\"\"\n",
    "    Runs training + test split + preprocessing + model training.\n",
    "    model_type: \"random_forest\", \"logistic\", \"xgboost\"\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Running: {title} ({model_type}) ===\")\n",
    "\n",
    "    # Split into features and target\n",
    "    X = df.drop(columns=[\"target_3class\"])\n",
    "    y = df[\"target_3class\"]\n",
    "    \n",
    "        # Convert target to numeric for XGBoost\n",
    "    if model_type == \"xgboost\":\n",
    "        y = y.astype(\"category\").cat.codes\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        stratify=y,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Build preprocessing based only on the feature matrix\n",
    "    preprocess = make_preprocess(X_train)\n",
    "\n",
    "    # Choose model\n",
    "    if model_type == \"random_forest\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=None,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_type == \"xgboost\":\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\"   # FAST + GPU compatible\n",
    "        )\n",
    "\n",
    "    elif model_type == \"logistic\":\n",
    "        model = LogisticRegression(max_iter=500)\n",
    "\n",
    "    # Create pipeline\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", preprocess),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Fit\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # ---------- Evaluation ----------\n",
    "    # תחזיות על קבוצת ה-test\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # 1. Accuracy – אחוז הדוגמאות שנחזו נכון\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # 2. Macro F1 – ממוצע F1 לכל המחלקות\n",
    "    macro_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Macro F1:\", macro_f1)\n",
    "\n",
    "    # 3. Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # 4. Classification Report – Precision / Recall / F1 לכל מחלקה\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return acc, pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9efa3e2ed2849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:30:36.064556Z",
     "start_time": "2025-11-29T18:19:13.951367100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================================\n",
      "MODEL: RANDOM_FOREST\n",
      "=============================================================\n",
      "\n",
      "=== Running: Full Clean Sample - random_forest (random_forest) ===\n",
      "Accuracy: 0.7725513545698125\n",
      "Macro F1: 0.3399933140145713\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   29    31  1752]\n",
      " [   11    84  1132]\n",
      " [   40    90 10267]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_paid       0.36      0.02      0.03      1812\n",
      "   paid_late       0.41      0.07      0.12      1227\n",
      "paid_on_time       0.78      0.99      0.87     10397\n",
      "\n",
      "    accuracy                           0.77     13436\n",
      "   macro avg       0.52      0.36      0.34     13436\n",
      "weighted avg       0.69      0.77      0.69     13436\n",
      "\n",
      "\n",
      "=== Running: Minimal Clean Sample - random_forest (random_forest) ===\n",
      "Accuracy: 0.79405\n",
      "Macro F1: 0.3172369946905172\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   62     4  2240]\n",
      " [    2    14  1849]\n",
      " [   11    13 15805]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_paid       0.83      0.03      0.05      2306\n",
      "   paid_late       0.45      0.01      0.01      1865\n",
      "paid_on_time       0.79      1.00      0.88     15829\n",
      "\n",
      "    accuracy                           0.79     20000\n",
      "   macro avg       0.69      0.34      0.32     20000\n",
      "weighted avg       0.77      0.79      0.71     20000\n",
      "\n",
      "\n",
      "=============================================================\n",
      "MODEL: XGBOOST\n",
      "=============================================================\n",
      "\n",
      "=== Running: Full Clean Sample - xgboost (xgboost) ===\n",
      "Accuracy: 0.7746353081274189\n",
      "Macro F1: 0.36576713814424694\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   53    41  1718]\n",
      " [   11   128  1088]\n",
      " [   55   115 10227]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.03      0.05      1812\n",
      "           1       0.45      0.10      0.17      1227\n",
      "           2       0.78      0.98      0.87     10397\n",
      "\n",
      "    accuracy                           0.77     13436\n",
      "   macro avg       0.56      0.37      0.37     13436\n",
      "weighted avg       0.71      0.77      0.70     13436\n",
      "\n",
      "\n",
      "=== Running: Minimal Clean Sample - xgboost (xgboost) ===\n",
      "Accuracy: 0.81545\n",
      "Macro F1: 0.5054897330834911\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  656    32  1618]\n",
      " [   34   253  1578]\n",
      " [  240   189 15400]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.28      0.41      2306\n",
      "           1       0.53      0.14      0.22      1865\n",
      "           2       0.83      0.97      0.89     15829\n",
      "\n",
      "    accuracy                           0.82     20000\n",
      "   macro avg       0.69      0.46      0.51     20000\n",
      "weighted avg       0.79      0.82      0.78     20000\n",
      "\n",
      "\n",
      "=============================================================\n",
      "MODEL: LOGISTIC\n",
      "=============================================================\n",
      "\n",
      "=== Running: Full Clean Sample - logistic (logistic) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhrqyyb/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.774188746650789\n",
      "Macro F1: 0.35031498946163175\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   61    26  1725]\n",
      " [   13    81  1133]\n",
      " [   76    61 10260]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_paid       0.41      0.03      0.06      1812\n",
      "   paid_late       0.48      0.07      0.12      1227\n",
      "paid_on_time       0.78      0.99      0.87     10397\n",
      "\n",
      "    accuracy                           0.77     13436\n",
      "   macro avg       0.56      0.36      0.35     13436\n",
      "weighted avg       0.70      0.77      0.69     13436\n",
      "\n",
      "\n",
      "=== Running: Minimal Clean Sample - logistic (logistic) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhrqyyb/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79915\n",
      "Macro F1: 0.5374162434009911\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  699    70  1537]\n",
      " [   71   503  1291]\n",
      " [  401   647 14781]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_paid       0.60      0.30      0.40      2306\n",
      "   paid_late       0.41      0.27      0.33      1865\n",
      "paid_on_time       0.84      0.93      0.88     15829\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.62      0.50      0.54     20000\n",
      "weighted avg       0.77      0.80      0.78     20000\n",
      "\n",
      "\n",
      "\n",
      "==================== SUMMARY ====================\n",
      "\n",
      "Model                Full Clean      Minimal Clean  \n",
      "--------------------------------------------------\n",
      "random_forest        0.7726          0.7941         \n",
      "xgboost              0.7746          0.8155         \n",
      "logistic             0.7742          0.7992         \n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# Run 3 models on both Full and Minimal datasets\n",
    "# =============================================================\n",
    "df_with_target = create_target(df_raw)\n",
    "\n",
    "df_sample = df_with_target.sample(100000, random_state=42)\n",
    "\n",
    "df_full_sample = full_clean(df_sample.copy())\n",
    "df_minimal_sample = minimal_clean(df_sample.copy())\n",
    "\n",
    "\n",
    "models = [\"random_forest\", \"xgboost\", \"logistic\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(\"\\n=============================================================\")\n",
    "    print(f\"MODEL: {model_name.upper()}\")\n",
    "    print(\"=============================================================\")\n",
    "\n",
    "    # ---- FULL CLEAN ----\n",
    "    acc_full, model_full = run_experiment(\n",
    "        df_full_sample,\n",
    "        f\"Full Clean Sample - {model_name}\",\n",
    "        model_type=model_name\n",
    "    )\n",
    "\n",
    "    # ---- MINIMAL CLEAN ----\n",
    "    acc_minimal, model_minimal = run_experiment(\n",
    "        df_minimal_sample,\n",
    "        f\"Minimal Clean Sample - {model_name}\",\n",
    "        model_type=model_name\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    results[(model_name, \"full\")] = acc_full\n",
    "    results[(model_name, \"minimal\")] = acc_minimal\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# Print Summary Table\n",
    "# =============================================================\n",
    "print(\"\\n\\n==================== SUMMARY ====================\\n\")\n",
    "print(\"{:<20} {:<15} {:<15}\".format(\"Model\", \"Full Clean\", \"Minimal Clean\"))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name in models:\n",
    "    print(\"{:<20} {:<15.4f} {:<15.4f}\".format(\n",
    "        model_name,\n",
    "        results[(model_name, \"full\")],\n",
    "        results[(model_name, \"minimal\")]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3129e9ae82e800",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-29T18:17:49.523300300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_3class\n",
       "paid_on_time    0.77121\n",
       "not_paid        0.13740\n",
       "paid_late       0.09139\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_sample[\"target_3class\"].value_counts()\n",
    "df_full_sample[\"target_3class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59fecbcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_3class\n",
       "paid_on_time    0.79142\n",
       "not_paid        0.11531\n",
       "paid_late       0.09327\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minimal_sample[\"target_3class\"].value_counts()\n",
    "df_minimal_sample[\"target_3class\"].value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
