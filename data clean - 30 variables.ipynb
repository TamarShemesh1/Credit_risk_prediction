{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63b2ce2722f41ba",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***Credit Risk Prediction: A Machine Learning Analysis of Lending Club Loans***\n",
    "\n",
    "A project by Anna Rakayev and Tamar Shemesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b2a0626aacc2a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:52:10.298506900Z",
     "start_time": "2025-11-14T10:52:07.563213Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad919081db4528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:52:49.367992100Z",
     "start_time": "2025-11-14T10:52:10.303844Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wn/wpy04pb15y934q92j5gbcvb80000gn/T/ipykernel_96963/3055991645.py:2: DtypeWarning: Columns (19,47,55,112,123,124,125,128,129,130,133,139,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"loan.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2260668, 145)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data\n",
    "df = pd.read_csv(\"loan.csv\")\n",
    "df.shape\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21906b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1073134135574087"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['last_pymnt_d'].isna().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_3class\n",
      "paid_on_time    831511\n",
      "not_paid        261686\n",
      "paid_late       210441\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Keep only relevant rows\n",
    "df_clean = df[df['loan_status'].isin(['Fully Paid', 'Charged Off', 'Default'])].copy()\n",
    "\n",
    "# Convert dates once (vectorized)\n",
    "df_clean['issue_d'] = pd.to_datetime(df_clean['issue_d'], format='%b-%Y', errors='coerce')\n",
    "df_clean['last_pymnt_d'] = pd.to_datetime(df_clean['last_pymnt_d'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "# Extract term in months\n",
    "df_clean['term_months'] = df_clean['term'].str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "# Expected loan end date\n",
    "df_clean['expected_end'] = df_clean['issue_d'] + pd.to_timedelta(df_clean['term_months'] * 30, unit='D')\n",
    "\n",
    "# Flag late fully paid\n",
    "df_clean['paid_late_flag'] = (\n",
    "    (df_clean['loan_status'] == 'Fully Paid') &\n",
    "    (df_clean['last_pymnt_d'] > df_clean['expected_end'])\n",
    ")\n",
    "\n",
    "# Build 3-class target\n",
    "df_clean['target_3class'] = 'paid_on_time'\n",
    "df_clean.loc[df_clean['paid_late_flag'], 'target_3class'] = 'paid_late'\n",
    "df_clean.loc[df_clean['loan_status'].isin(['Charged Off', 'Default']), 'target_3class'] = 'not_paid'\n",
    "\n",
    "# Check distribution\n",
    "print(df_clean['target_3class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22dec5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(950, 150)\n"
     ]
    }
   ],
   "source": [
    "df_clean['days_late'] = (df_clean['last_pymnt_d'] - df_clean['expected_end']).dt.days\n",
    "\n",
    "late_fully_paid = df_clean[\n",
    "    (df_clean['loan_status'] == 'Fully Paid') &\n",
    "    (df_clean['paid_late_flag'] == True) &\n",
    "    (df_clean['target_3class'] == 'paid_late') &\n",
    "    (df_clean['days_late'] > 100)\n",
    "]\n",
    "\n",
    "print(late_fully_paid.shape)\n",
    "late_fully_paid[['issue_d', 'expected_end', 'last_pymnt_d', 'days_late', 'loan_status', 'target_3class']].head(10)\n",
    "\n",
    "\n",
    "helper_cols_to_drop = [\n",
    "    'term_months',\n",
    "    'expected_end',\n",
    "    'paid_late_flag',\n",
    "    'last_pymnt_d',\n",
    "]\n",
    "\n",
    "df_clean = df_clean.drop(columns=[c for c in helper_cols_to_drop if c in df_clean.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb900049c4eedd5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***Handling Correlation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f573d207ee587026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:53:46.985910900Z",
     "start_time": "2025-11-14T10:52:52.090579800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated numeric column pairs (corr > 0.9):\n",
      "out_prncp                      ↔ out_prncp_inv                   corr = 1.00\n",
      "hardship_amount                ↔ orig_projected_additional_accrued_interest  corr = 1.00\n",
      "funded_amnt                    ↔ loan_amnt                       corr = 1.00\n",
      "total_pymnt                    ↔ total_pymnt_inv                 corr = 1.00\n",
      "funded_amnt                    ↔ funded_amnt_inv                 corr = 1.00\n",
      "num_sats                       ↔ open_acc                        corr = 1.00\n",
      "funded_amnt_inv                ↔ loan_amnt                       corr = 1.00\n",
      "num_actv_rev_tl                ↔ num_rev_tl_bal_gt_0             corr = 0.98\n",
      "tot_cur_bal                    ↔ tot_hi_cred_lim                 corr = 0.97\n",
      "collection_recovery_fee        ↔ recoveries                      corr = 0.97\n",
      "total_pymnt                    ↔ total_rec_prncp                 corr = 0.97\n",
      "total_pymnt_inv                ↔ total_rec_prncp                 corr = 0.97\n",
      "total_bal_il                   ↔ total_il_high_credit_limit      corr = 0.96\n",
      "funded_amnt                    ↔ installment                     corr = 0.95\n",
      "installment                    ↔ loan_amnt                       corr = 0.95\n",
      "funded_amnt_inv                ↔ installment                     corr = 0.95\n",
      "hardship_payoff_balance_amount ↔ settlement_amount               corr = 0.95\n",
      "total_bal_ex_mort              ↔ total_bal_il                    corr = 0.90\n",
      "\n",
      "Columns involved in high correlations (>0.9):\n",
      "['collection_recovery_fee', 'funded_amnt', 'funded_amnt_inv', 'hardship_amount', 'hardship_payoff_balance_amount', 'installment', 'loan_amnt', 'num_actv_rev_tl', 'num_rev_tl_bal_gt_0', 'num_sats', 'open_acc', 'orig_projected_additional_accrued_interest', 'out_prncp', 'out_prncp_inv', 'recoveries', 'settlement_amount', 'tot_cur_bal', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bal_il', 'total_il_high_credit_limit', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp']\n"
     ]
    }
   ],
   "source": [
    "# --- Correlation check (numeric columns only) ---\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = df_clean.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Compute absolute correlation matrix\n",
    "corr = df_clean[numeric_cols].corr().abs()\n",
    "\n",
    "# Find highly correlated pairs (r > 0.9)\n",
    "high_corr_pairs = [\n",
    "    (c1, c2, corr.loc[c1, c2])\n",
    "    for c1 in corr.columns\n",
    "    for c2 in corr.columns\n",
    "    if c1 != c2 and corr.loc[c1, c2] > 0.9\n",
    "]\n",
    "\n",
    "# Sort by correlation strength and show if > 0.9\n",
    "high_corr_pairs = sorted(list(set(tuple(sorted(p[:2])) for p in high_corr_pairs)), key=lambda x: corr.loc[x[0], x[1]], reverse=True)\n",
    "print(\"Highly correlated numeric column pairs (corr > 0.9):\")\n",
    "for c1, c2 in high_corr_pairs[:30]:\n",
    "    print(f\"{c1:30} ↔ {c2:30}  corr = {corr.loc[c1, c2]:.2f}\")\n",
    "\n",
    "\n",
    "# Extract a flat list of columns that appear in high-correlation pairs\n",
    "high_corr_cols = sorted({c for c1, c2 in high_corr_pairs for c in (c1, c2)})\n",
    "print(\"\\nColumns involved in high correlations (>0.9):\")\n",
    "print(high_corr_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e0b067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                            100.000000\n",
      "url                                           100.000000\n",
      "member_id                                     100.000000\n",
      "next_pymnt_d                                   99.997622\n",
      "orig_projected_additional_accrued_interest     99.736660\n",
      "hardship_end_date                              99.590761\n",
      "hardship_status                                99.590761\n",
      "deferral_term                                  99.590761\n",
      "hardship_amount                                99.590761\n",
      "hardship_start_date                            99.590761\n",
      "hardship_dpd                                   99.590761\n",
      "payment_plan_start_date                        99.590761\n",
      "hardship_length                                99.590761\n",
      "hardship_loan_status                           99.590761\n",
      "hardship_payoff_balance_amount                 99.590761\n",
      "hardship_last_payment_amount                   99.590761\n",
      "hardship_reason                                99.590761\n",
      "hardship_type                                  99.590761\n",
      "sec_app_mths_since_last_major_derog            99.544659\n",
      "sec_app_revol_util                             98.755559\n",
      "revol_bal_joint                                98.733084\n",
      "sec_app_num_rev_accts                          98.733007\n",
      "sec_app_open_act_il                            98.733007\n",
      "sec_app_chargeoff_within_12_mths               98.733007\n",
      "sec_app_open_acc                               98.733007\n",
      "sec_app_mort_acc                               98.733007\n",
      "sec_app_inq_last_6mths                         98.733007\n",
      "sec_app_earliest_cr_line                       98.733007\n",
      "sec_app_collections_12_mths_ex_med             98.733007\n",
      "verification_status_joint                      98.228189\n",
      "dti_joint                                      98.217143\n",
      "annual_inc_joint                               98.216990\n",
      "debt_settlement_flag_date                      97.545868\n",
      "settlement_status                              97.545868\n",
      "settlement_date                                97.545868\n",
      "settlement_amount                              97.545868\n",
      "settlement_percentage                          97.545868\n",
      "settlement_term                                97.545868\n",
      "desc                                           90.539935\n",
      "dtype: float64\n",
      "['id', 'url', 'member_id', 'next_pymnt_d', 'orig_projected_additional_accrued_interest', 'hardship_end_date', 'hardship_status', 'deferral_term', 'hardship_amount', 'hardship_start_date', 'hardship_dpd', 'payment_plan_start_date', 'hardship_length', 'hardship_loan_status', 'hardship_payoff_balance_amount', 'hardship_last_payment_amount', 'hardship_reason', 'hardship_type', 'sec_app_mths_since_last_major_derog', 'sec_app_revol_util', 'revol_bal_joint', 'sec_app_num_rev_accts', 'sec_app_open_act_il', 'sec_app_chargeoff_within_12_mths', 'sec_app_open_acc', 'sec_app_mort_acc', 'sec_app_inq_last_6mths', 'sec_app_earliest_cr_line', 'sec_app_collections_12_mths_ex_med', 'verification_status_joint', 'dti_joint', 'annual_inc_joint', 'debt_settlement_flag_date', 'settlement_status', 'settlement_date', 'settlement_amount', 'settlement_percentage', 'settlement_term', 'desc']\n"
     ]
    }
   ],
   "source": [
    "# אחוזי חוסרים לכל עמודה\n",
    "missing_percent = df_clean.isnull().mean() * 100\n",
    "\n",
    "# עמודות עם יותר מ-90% חוסרים\n",
    "high_missing_90 = missing_percent[missing_percent > 90].sort_values(ascending=False)\n",
    "\n",
    "print(high_missing_90)\n",
    "high_missing_cols_90 = list(high_missing_90.index)\n",
    "\n",
    "print(high_missing_cols_90)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ca03719bac83a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:55:41.838516800Z",
     "start_time": "2025-11-14T10:55:41.816010800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Columns that contain post-loan information \n",
    "leakage_columns = [\n",
    "    'hardship_flag', 'debt_settlement_flag',\n",
    "    'total_pymnt', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee',\n",
    "    'last_pymnt_d', 'last_pymnt_amnt', 'recoveries',\n",
    "    'collection_recovery_fee', 'out_prncp', 'total_pymnt_inv', 'out_prncp_inv'\n",
    "]\n",
    "\n",
    "#not important and highly correlated\n",
    "high_corr_drop_columns = [\n",
    "    'funded_amnt', 'funded_amnt_inv', 'installment',\n",
    "    'num_rev_tl_bal_gt_0', 'tot_hi_cred_lim',\n",
    "    'total_il_high_credit_limit','num_sats'\n",
    "]\n",
    "\n",
    "#zero variance\n",
    "redundant_columns = ['policy_code','disbursement_method','chargeoff_within_12_mths']\n",
    "\n",
    "#didnt remove \n",
    "not_leakage = ['grade', 'sub_grade', 'issue_d']\n",
    "important_columns = ['total_rev_hi_lim', 'initial_list_status']\n",
    "check_before_remove = ['revol_bal', 'open_acc', 'title', 'last_credit_pull_d']\n",
    "high_corr_but_relevant = ['total_bal_ex_mort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f99724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Cleaning function used inside the Pipeline ----------\n",
    "def basic_clean1(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    # 1. deleting leakage columns + high correlation columns + zero variance columns\n",
    "    cols_to_drop_fixed = [c for c in (leakage_columns + high_corr_drop_columns + redundant_columns) if c in clean_df.columns]\n",
    "    clean_df = clean_df.drop(columns=cols_to_drop_fixed)\n",
    "\n",
    "    # 2. deleting columns with more than 90% missing values\n",
    "    missing_percent = clean_df.isnull().mean() * 100\n",
    "    high_missing_cols_90 = missing_percent[missing_percent > 90].index.tolist()\n",
    "    clean_df = clean_df.drop(columns=[c for c in high_missing_cols_90 if c in clean_df.columns])\n",
    "\n",
    "    # 3. removing 'Not Verified' from verification_status\n",
    "    clean_df = clean_df[clean_df['verification_status'] != 'Not Verified'].reset_index(drop=True)\n",
    "    clean_df = clean_df.drop(columns=['verification_status'])\n",
    "\n",
    "    # 4. cleaning term – keep only the number (36, 60, etc.)\n",
    "    clean_df['term'] = (clean_df['term'].astype(str).str.extract(r'(\\d+)')[0].astype(float))\n",
    "\n",
    "    # 5. cleaning emp_length\n",
    "    emp = clean_df['emp_length'].astype(str)\n",
    "    emp = emp.str.replace('< 1', '0', regex=False)\n",
    "    emp = emp.str.extract(r'(\\d+)')[0]\n",
    "    clean_df['emp_length'] = emp.astype(float)\n",
    "\n",
    "    # 6. removing rows with missing target_3class\n",
    "    clean_df = clean_df[clean_df['target_3class'].notna()].reset_index(drop=True)\n",
    "        \n",
    "    return clean_df\n",
    "\n",
    "\n",
    "df_clean = basic_clean1(df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6835ad05752e795c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymnt_plan\n",
      "n    99.969832\n",
      "y     0.030168\n",
      "Name: proportion, dtype: float64\n",
      "zip_code unique: 956\n",
      "collections_12_mths_ex_med zeros(%): 98.33752678411868\n",
      "application_type\n",
      "Individual    94.660428\n",
      "Joint App      5.339572\n",
      "Name: proportion, dtype: float64\n",
      "tot_coll_amt missing(%): 3.1086386855566586\n",
      "tot_coll_amt zeros(%): 85.21397215336351\n",
      "           num_sats  total_acc\n",
      "num_sats   1.000000   0.715618\n",
      "total_acc  0.715618   1.000000\n"
     ]
    }
   ],
   "source": [
    "#checking data before removing non informative columns\n",
    "# pymnt_plan – check distribution (if almost all 'n' → not informative)\n",
    "print(df['pymnt_plan'].value_counts(dropna=False, normalize=True) * 100)\n",
    "\n",
    "# zip_code –  many unique values = high-cardinality\n",
    "print(\"zip_code unique:\", df['zip_code'].nunique())\n",
    "\n",
    "# collections_12_mths_ex_med – check % of zeros (very high → not informative)\n",
    "print(\"collections_12_mths_ex_med zeros(%):\", (df['collections_12_mths_ex_med'] == 0).mean() * 100)\n",
    "\n",
    "# application_type – check category balance ( 'INDIVIDUAL' 94.7% - leave for now)\n",
    "print(df['application_type'].value_counts(dropna=False, normalize=True) * 100)\n",
    "\n",
    "# tot_coll_amt – check missing and zeros 3% missing, 85% zeros - leave for now\n",
    "print(\"tot_coll_amt missing(%):\", df['tot_coll_amt'].isna().mean() * 100)\n",
    "print(\"tot_coll_amt zeros(%):\", (df['tot_coll_amt'].fillna(0) == 0).mean() * 100)\n",
    "\n",
    "# num_sats – check correlation with total_acc (if r≈1.0 → redundant)\n",
    "print(df[['num_sats', 'total_acc']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd0c31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_map = {\n",
    "        'me': 'northeast', 'nh': 'northeast', 'vt': 'northeast', 'ma': 'northeast',\n",
    "        'ri': 'northeast', 'ct': 'northeast', 'ny': 'northeast', 'nj': 'northeast', 'pa': 'northeast',\n",
    "        'oh': 'midwest', 'in': 'midwest', 'il': 'midwest', 'mi': 'midwest', 'wi': 'midwest',\n",
    "        'mn': 'midwest', 'ia': 'midwest', 'mo': 'midwest', 'nd': 'midwest', 'sd': 'midwest',\n",
    "        'ne': 'midwest', 'ks': 'midwest',\n",
    "        'de': 'south', 'md': 'south', 'dc': 'south', 'va': 'south', 'wv': 'south',\n",
    "        'nc': 'south', 'sc': 'south', 'ga': 'south', 'fl': 'south',\n",
    "        'ky': 'south', 'tn': 'south', 'al': 'south', 'ms': 'south',\n",
    "        'ar': 'south', 'la': 'south', 'tx': 'south', 'ok': 'south',\n",
    "        'mt': 'west', 'id': 'west', 'wy': 'west', 'co': 'west', 'nm': 'west',\n",
    "        'az': 'west', 'ut': 'west', 'nv': 'west', 'wa': 'west',\n",
    "        'or': 'west', 'ca': 'west', 'hi': 'west', 'ak': 'west',\n",
    "    }\n",
    "\n",
    "df_clean['state_region'] = df_clean['addr_state'].map(region_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96dd6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# deleting non informative columns + simplifying columns\n",
    "def basic_clean2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    # remove non-informative\n",
    "    non_informative = [\n",
    "        'pymnt_plan',\n",
    "        'zip_code',\n",
    "        'collections_12_mths_ex_med',\n",
    "    ]\n",
    "    clean_df = clean_df.drop(columns=[c for c in non_informative if c in clean_df.columns])\n",
    "\n",
    "    # clean text columns\n",
    "    cols_to_clean = ['purpose', 'home_ownership', 'addr_state', 'application_type', 'emp_title']\n",
    "    for c in cols_to_clean:\n",
    "        clean_df[c] = clean_df[c].astype(str).str.lower().str.strip()\n",
    "\n",
    "    # grouped purpose\n",
    "    clean_df['purpose_grouped'] = clean_df['purpose'].replace({\n",
    "        'debt_consolidation': 'debt',\n",
    "        'credit_card': 'debt',\n",
    "        'home_improvement': 'housing',\n",
    "        'house': 'housing',\n",
    "        'small_business': 'business',\n",
    "        'car': 'personal',\n",
    "        'medical': 'personal',\n",
    "        'vacation': 'personal',\n",
    "        'moving': 'personal',\n",
    "        'wedding': 'personal',\n",
    "        'major_purchase': 'personal',\n",
    "        'renewable_energy': 'other',\n",
    "        'educational': 'other',\n",
    "        'other': 'other'\n",
    "    })\n",
    "\n",
    "    # home stability\n",
    "    clean_df['home_stability'] = clean_df['home_ownership'].replace({\n",
    "        'mortgage': 'stable',\n",
    "        'own': 'stable',\n",
    "        'rent': 'unstable',\n",
    "        'none': 'unstable',\n",
    "        'other': 'unstable'\n",
    "    })\n",
    "\n",
    "    # credit age\n",
    "    years = (clean_df['earliest_cr_line'].astype(str).str.extract(r'(\\d{4})')[0].astype(float))\n",
    "    clean_df['credit_age_years'] = datetime.now().year - years\n",
    "\n",
    "    # recorded public issues\n",
    "    clean_df['bad_records_count'] = clean_df[['pub_rec', 'pub_rec_bankruptcies', 'tax_liens']].sum(axis=1, min_count=1)\n",
    "\n",
    "    # recent credit activity\n",
    "    clean_df['recent_credit_activity'] = (\n",
    "        clean_df['inq_last_6mths']\n",
    "        + clean_df['num_tl_op_past_12m']\n",
    "        - (clean_df['mths_since_recent_inq'] / 12)\n",
    "    )\n",
    "\n",
    "    # combined balances\n",
    "    clean_df['total_balance_all'] = clean_df['tot_cur_bal'] + clean_df['total_bal_il']\n",
    "\n",
    "    # combined active accounts\n",
    "    clean_df['active_credit_accounts'] = clean_df['num_actv_bc_tl'] + clean_df['num_actv_rev_tl']\n",
    "\n",
    "    # delinquency severity (any delinquency indicator)\n",
    "    clean_df['any_delinquency'] = (\n",
    "        (clean_df['num_accts_ever_120_pd'] > 0) |\n",
    "        (clean_df['num_tl_120dpd_2m'] > 0) |\n",
    "        (clean_df['num_tl_90g_dpd_24m'] > 0) |\n",
    "        (clean_df['num_tl_30dpd'] > 0) |\n",
    "        (clean_df['delinq_2yrs'] > 0)\n",
    "    ).astype(int)\n",
    "\n",
    "\n",
    "    clean_df['is_joint_app'] = (clean_df['application_type'].str.contains('joint')).astype(int)\n",
    "    clean_df['has_current_delinquency'] = (clean_df['acc_now_delinq'] > 0).astype(int)\n",
    "    clean_df['has_collections'] = (clean_df['tot_coll_amt'] > 0).astype(int)\n",
    "\n",
    "\n",
    "    # columns that have been used (remove raw columns after creating features)\n",
    "    columns_to_remove = [\n",
    "    # inputs for engineered features\n",
    "    'pub_rec', 'pub_rec_bankruptcies', 'tax_liens',\n",
    "    'inq_last_6mths', 'num_tl_op_past_12m', 'mths_since_recent_inq',\n",
    "    'tot_cur_bal', 'total_bal_il',\n",
    "    'num_actv_bc_tl', 'num_actv_rev_tl',\n",
    "    'num_accts_ever_120_pd', 'num_tl_120dpd_2m',\n",
    "    'num_tl_90g_dpd_24m', 'delinq_2yrs', 'num_tl_30dpd',\n",
    "\n",
    "    # raw cols replaced by new engineered features\n",
    "    'purpose',\n",
    "    'home_ownership',\n",
    "    'earliest_cr_line',\n",
    "    'application_type',\n",
    "    'acc_now_delinq',\n",
    "    'tot_coll_amt',\n",
    "\n",
    "    # additional raw / text / redundant columns\n",
    "    'title',\n",
    "    'emp_title',\n",
    "    'addr_state',\n",
    "    'issue_d',\n",
    "    'last_credit_pull_d',\n",
    "    'loan_status',\n",
    "    'days_late',\n",
    "    'open_acc',\n",
    "    'revol_bal'\n",
    "]\n",
    "    \n",
    "    \n",
    "    clean_df = clean_df.drop(columns=columns_to_remove)\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "\n",
    "df_clean = basic_clean2(df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low predictive power – features with minimal contribution to credit risk prediction\n",
    "def basic_clean3(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_clean = df.copy()\n",
    "    low_importance_features = [\n",
    "        'open_act_il',             # rarely informative, few borrowers have active installment accounts\n",
    "        'open_il_12m',             # very sparse – number of installment loans opened last 12 months\n",
    "        'open_il_24m',             # similar to above; low predictive value\n",
    "        'open_rv_12m',             # sparse revolving account activity – unstable feature\n",
    "        'open_rv_24m',             # redundant and weak\n",
    "        'open_acc_6m',             # number of accounts opened last 6 months – unstable, noisy\n",
    "        'inq_fi',                  # financial inquiries – almost always zero\n",
    "        'total_cu_tl',             # credit union trades – rare, low impact\n",
    "        'acc_open_past_24mths'     # redundant with more useful credit activity indicators\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Redundant or highly correlated features – information already captured by engineered features\n",
    "    redundant_features = [\n",
    "        'num_bc_sats',             # count of satisfactory bankcard accounts – correlated with num_bc_tl\n",
    "        'num_bc_tl',               # total bankcard trade lines – highly correlated with other credit totals\n",
    "        'num_il_tl',               # total installment accounts – redundant with derived totals\n",
    "        'num_op_rev_tl',           # revolving accounts – captured by active_credit_accounts\n",
    "        'num_rev_accts',           # total revolving accounts – redundant, high correlation\n",
    "        'total_acc'                # total credit accounts – overly broad, correlated with totals\n",
    "    ]\n",
    "\n",
    "    # Sparse or missing-heavy features – too many NaN values or extremely rare events\n",
    "    sparse_features = [\n",
    "        'mths_since_last_record',          # many missing, very rare credit events\n",
    "        'mths_since_recent_bc_dlq',        # very sparse delinquency timing\n",
    "        'mths_since_recent_revol_delinq',  # rarely populated\n",
    "        'percent_bc_gt_75'                 # unstable distribution, extreme sparsity\n",
    "    ]\n",
    "\n",
    "    # Noisy or unstable features – inconsistent behavior, low reliability\n",
    "    noisy_features = [\n",
    "        'mo_sin_old_il_acct',      # months since oldest installment account – noisy\n",
    "        'mo_sin_old_rev_tl_op',    # months since oldest revolving account – correlated & unstable\n",
    "        'mo_sin_rcnt_rev_tl_op',   # recent revolving age – too volatile\n",
    "        'mo_sin_rcnt_tl',          # age of recent trade line – noisy, redundant\n",
    "        'mths_since_rcnt_il'       # months since recent installment loan – sparse/unstable\n",
    "    ]\n",
    "\n",
    "    columns_optional = [\n",
    "    'mths_since_last_delinq', 'avg_cur_bal', 'max_bal_bc', 'all_util',\n",
    "    'il_util', 'inq_last_12m', 'pct_tl_nvr_dlq',\n",
    "    'mort_acc', 'total_bc_limit', 'total_acc'\n",
    "]\n",
    "\n",
    "    # Combine all removable feature groups\n",
    "    features_to_remove = (\n",
    "        low_importance_features +\n",
    "        redundant_features +\n",
    "        sparse_features +\n",
    "        noisy_features+\n",
    "        columns_optional\n",
    "    )\n",
    "\n",
    "    df_clean = df_clean.drop(columns=[c for c in features_to_remove if c in df_clean.columns])\n",
    "\n",
    "    if 'target_3class' in clean_df.columns:\n",
    "        clean_df = clean_df[clean_df['target_3class'].notna()].reset_index(drop=True)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "df_clean = basic_clean3(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "555a057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Index(['loan_amnt', 'term', 'int_rate', 'grade', 'sub_grade', 'emp_length',\n",
      "       'annual_inc', 'dti', 'revol_util', 'initial_list_status',\n",
      "       'mths_since_last_major_derog', 'total_rev_hi_lim', 'bc_open_to_buy',\n",
      "       'bc_util', 'delinq_amnt', 'mths_since_recent_bc', 'total_bal_ex_mort',\n",
      "       'target_3class', 'state_region', 'purpose_grouped', 'home_stability',\n",
      "       'credit_age_years', 'bad_records_count', 'recent_credit_activity',\n",
      "       'total_balance_all', 'active_credit_accounts', 'any_delinquency',\n",
      "       'is_joint_app', 'has_current_delinquency', 'has_collections'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(len(df_clean.columns))\n",
    "print(df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56f3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
